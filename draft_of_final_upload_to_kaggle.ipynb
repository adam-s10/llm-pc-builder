{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Remove conflicting packages from the base environment.\n",
        "!pip uninstall -qqy kfp jupyterlab libpysal thinc spacy fastai ydata-profiling google-cloud-bigquery google-generativeai\n",
        "\n",
        "# Install langgraph and the packages needed.\n",
        "!pip install -qU 'langgraph==0.3.21' 'langchain-google-genai==2.1.2' 'langgraph-prebuilt==0.1.7'"
      ],
      "metadata": {
        "id": "8iixR858rYci"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Imports\n",
        "import os\n",
        "import json\n",
        "import re\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "from google import genai\n",
        "from google.genai import types\n",
        "from google.api_core import retry\n",
        "\n",
        "import chromadb\n",
        "from chromadb import EmbeddingFunction, Embeddings, Documents\n",
        "\n",
        "from pprint import pprint\n",
        "from typing import List, Dict, Any, Annotated, Literal, Optional\n",
        "from collections.abc import Iterable\n",
        "\n",
        "\n",
        "from typing_extensions import TypedDict\n",
        "from IPython.display import Image\n",
        "\n",
        "from langchain_core.tools import tool\n",
        "from langchain_core.documents import Document\n",
        "from langchain_core.messages import AIMessage\n",
        "from langchain_core.messages.tool import ToolMessage\n",
        "\n",
        "from langgraph.prebuilt import ToolNode\n",
        "from langgraph.graph.message import add_messages\n",
        "from langgraph.graph import StateGraph, START, END\n",
        "\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from langchain.document_loaders import TextLoader\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter"
      ],
      "metadata": {
        "id": "v0pyUghDtXe0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "load_dotenv()\n",
        "\n",
        "# Setup API key for Gemini\n",
        "os.environ['GOOGLE_API_KEY'] = os.getenv('GOOGLE_API_KEY')\n",
        "GOOGLE_API_KEY = os.getenv('GOOGLE_API_KEY')\n",
        "\n",
        "CHROMA_PATH = r'part_selection_db' # TODO: change this to Kaggle path on upload!!!\n",
        "DATA_PATH = 'txt_data/' # TODO: change this to Kaggle path on upload!!!\n",
        "chroma_client = chromadb.PersistentClient(CHROMA_PATH)"
      ],
      "metadata": {
        "id": "j4V2xiyOtQ7O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class GeminiEmbeddingFunction(EmbeddingFunction):\n",
        "    # Specify whether to generate embeddings for documents, or queries\n",
        "    document_mode = True\n",
        "\n",
        "    # Define a helper to retry when per-minute quota is reached.\n",
        "    is_retriable = lambda e: (isinstance(e, genai.errors.APIError) and e.code in {429, 503})\n",
        "\n",
        "    @retry.Retry(predicate=is_retriable)\n",
        "    def __call__(self, input_: Documents) -> Embeddings:\n",
        "        if self.document_mode:\n",
        "            embedding_task = 'retrieval_document'\n",
        "        else:\n",
        "            embedding_task = 'retrieval_query'\n",
        "\n",
        "        response = genai.Client(api_key=os.getenv('GOOGLE_API_KEY')).models.embed_content(\n",
        "            model='models/text-embedding-004',\n",
        "            contents=input_,\n",
        "            config=types.EmbedContentConfig(\n",
        "                task_type=embedding_task,\n",
        "            ),\n",
        "        )\n",
        "        return [em.values for em in response.embeddings]"
      ],
      "metadata": {
        "id": "q74AetLptNbR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_extracted_text(text: str) -> str:\n",
        "    new_text = text.replace('~', '')\n",
        "    new_text = new_text.replace('©', '')\n",
        "    new_text = new_text.replace('_', '')\n",
        "    new_text = new_text.replace(';:;', '')\n",
        "    new_text = new_text.replace('®', '')\n",
        "    new_text = new_text.replace('#', '')\n",
        "    new_text = new_text.replace('@', '')\n",
        "    new_text = new_text.replace(' ', '')\n",
        "    return new_text"
      ],
      "metadata": {
        "id": "34oZVOuStLN7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_chunks(file_path: str) -> List[Document]:\n",
        "    loader = TextLoader(file_path)\n",
        "    raw_documents = loader.load()\n",
        "    print(raw_documents)\n",
        "\n",
        "    # splitting the document\n",
        "    text_splitter = RecursiveCharacterTextSplitter(\n",
        "        chunk_size=500,\n",
        "        chunk_overlap=60,\n",
        "        length_function=len,\n",
        "        is_separator_regex=False,\n",
        "    )\n",
        "    chunks = text_splitter.split_documents(raw_documents)\n",
        "    print(chunks)\n",
        "    return chunks"
      ],
      "metadata": {
        "id": "fwvC3qeZtJtV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def batch_embed_chunks(file_path: str, chunks: List[Document], batch_size: int=50) -> None:\n",
        "    # Process chunks in smaller batches to manage resources and API limits\n",
        "    for i in range(0, len(chunks), batch_size):\n",
        "        batch_chunks = chunks[i:i + batch_size]\n",
        "        batch_documents = [clean_extracted_text(chunk.page_content) for chunk in batch_chunks]\n",
        "\n",
        "        try:\n",
        "            batch_embeddings = embed_fn(batch_documents)\n",
        "        except Exception as exc:\n",
        "            print(f'Error generating embeddings for batch {i // batch_size} from {file_path}: {exc}')\n",
        "            continue\n",
        "\n",
        "        batch_metadata = [chunk.metadata for chunk in batch_chunks]\n",
        "        batch_ids = [\n",
        "            f'{os.path.splitext(file)[0]}_chunk_{i + j}'\n",
        "            for j in range(len(batch_chunks))\n",
        "        ]\n",
        "\n",
        "        db.add(\n",
        "            documents=batch_documents,\n",
        "            metadatas=batch_metadata,\n",
        "            ids=batch_ids\n",
        "        )\n",
        "        print(f'Added batch {i // batch_size + 1} from {file_path} to ChromaDB.  Total: {db.count()}')"
      ],
      "metadata": {
        "id": "1deZrG__tGy3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embed_fn = GeminiEmbeddingFunction()\n",
        "embed_fn.document_mode = True\n",
        "db = chroma_client.get_or_create_collection(name='building_pcs', embedding_function=embed_fn)\n",
        "\n",
        "for file in os.listdir(DATA_PATH):\n",
        "    if not file.endswith('.txt'):\n",
        "        continue\n",
        "\n",
        "    print('File being worked on:', file)\n",
        "    path = os.path.join(DATA_PATH, file)\n",
        "\n",
        "    c = get_chunks(path)\n",
        "    batch_embed_chunks(path, c)\n",
        "\n",
        "embed_fn.document_mode = False\n",
        "\n"
      ],
      "metadata": {
        "id": "rZ1sIkw7s--J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_query_result(query: str) -> list[str]:\n",
        "    fn = GeminiEmbeddingFunction()\n",
        "    fn.document_mode = False\n",
        "    r = chroma_client.get_collection(name='building_pcs', embedding_function=fn).query(query_texts=[query],\n",
        "                                                                                         n_results=5)\n",
        "    [ans] = r['documents']\n",
        "    return ans"
      ],
      "metadata": {
        "id": "VlIMiT5ps8mC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class PCBuilderState(TypedDict):\n",
        "    \"\"\"State representing the PC builder conversation.\"\"\"\n",
        "\n",
        "    # The chat conversation. This preserves the conversation history\n",
        "    # between nodes. The `add_messages` annotation indicates to LangGraph\n",
        "    # that state is updated by appending returned messages, not replacing\n",
        "    # them.\n",
        "    messages: Annotated[list, add_messages]\n",
        "\n",
        "    # User requirements\n",
        "    # requirements: Dict[str, Any]\n",
        "    requirements: list[str]\n",
        "\n",
        "    # Budget information\n",
        "    budget: str\n",
        "\n",
        "    # Current recommendations for parts\n",
        "    recommendations: list[Dict[str, Any]]\n",
        "\n",
        "    # Flag for whether the build is complete\n",
        "    build_complete: bool\n",
        "\n",
        "    # Flag to indicate waiting for user input\n",
        "    # waiting_for_user: bool"
      ],
      "metadata": {
        "id": "fslYvelWs6Cn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# The system instruction defines how the chatbot is expected to behave and includes\n",
        "# rules for when to call different functions, as well as rules for the conversation, such\n",
        "# as tone and what is permitted for discussion.\n",
        "PC_BUILDER_SYSINT = (\n",
        "    'system',  # 'system' indicates the message is a system instruction.\n",
        "    '''\n",
        "You are a PC Builder Assistant, an expert in computer hardware and building custom PCs.\n",
        "Your goal is to help users find the perfect PC build based on their budget and requirements.\n",
        "\n",
        "You should:\n",
        "1. Ask about their budget and use case (gaming, office work, content creation, etc.)\n",
        "2. Determine if they need a custom build or pre-built system.\n",
        "3. For pre-built systems, ask if desktops or laptops or either are preferred.\n",
        "4. For custom builds, if they have existing hardware, ask what parts they want to upgrade.\n",
        "\n",
        "Once you have a general idea of what the user is looking for, you must formulate that\n",
        "into a structured list of concise individual requirements and call the update_plans tool,\n",
        "providing the budget and list of requirements. For custom build devices, if the user\n",
        "mentions wanting the PC for a specific task that is hardware intensive, such as playing a\n",
        "video game or video editing or training AI models or such, then first use the\n",
        "search_task_requirements tool to find the hardware requirements for the specified task,\n",
        "and only then update the user requirements using the update_plans tool, with the extra\n",
        "hardware requirements added to it.\n",
        "\n",
        "For pre-builds, even if there are potentially-intensive task requirements, you can call\n",
        "the update_plans tool directly, adding the task requirements as part of the main requirements.\n",
        "\n",
        "Once the requirements are all logged by the tool, summarize the requirements back to the\n",
        "user in a couple of sentences using the latest requirements list and budget returned by\n",
        "your last call of the tool.\n",
        "\n",
        "If the user wants to change something in their plans, send a new list of requirements back\n",
        "to update_plans. If they want to start from scratch, use the clear_plan tool to remove all\n",
        "budget and requirements, then walk through the requirements gathering steps again.\n",
        "\n",
        "If the user confirms their requirements are final and they want a pre-built device, use\n",
        "search_prebuilt tool to find pre-built devices fitting the user criteria, which will be\n",
        "given to you in a structured JSON format. If the JSON has formatting errors and you cannot\n",
        "understand it, recall the search_prebuilt tool. If the JSON can be understood, call the\n",
        "rank_prebuilds tool and provide it the exact same JSON (it will not work without JSON!) to\n",
        "get a final list of recommendations and then render this neatly in markdown for the user\n",
        "to browse.\n",
        "\n",
        "If the user confirms their requirements are final and they want a custom build, use the\n",
        "lookup_parts_needed tool to get a list of the parts required. Once you have the parts,\n",
        "call the search_custom_parts tool to get a list of parts, which should be in a structured\n",
        "JSON format. If the JSON has formatting errors and you cannot understand it, recall the\n",
        "search_custom_parts tool. If the JSON can be understood, call the rank_parts tool and\n",
        "provide it the exact same JSON (it will not work without JSON!) to get a final list of\n",
        "parts for the user. Render this neatly in markdown for the user to browse.\n",
        "\n",
        "The user may have additional questions about the parts or building process, which you must\n",
        "expand upon if asked.\n",
        "\n",
        "If any of the tools are unavailable, let the user know instead of trying to call the tools.\n",
        "\n",
        "Stay focused on PC building. If users ask about unrelated topics, gently redirect them.\n",
        "''',\n",
        ")\n",
        "\n",
        "# This is the message with which the system opens the conversation.\n",
        "WELCOME_MSG = ('''\n",
        "Welcome to the PC Builder Assistant! (Type `q` to quit). I'll help you find the perfect computer\n",
        "based on your needs and budget. Could you tell me your budget and what you'll be using this PC for? (Gaming, office\n",
        "work, content creation, etc.)\n",
        "''')\n",
        "\n",
        "# LLM model definitions\n",
        "MAIN_MODEL = 'gemini-2.0-flash'\n",
        "llm = ChatGoogleGenerativeAI(model=MAIN_MODEL)\n",
        "client = genai.Client(api_key=os.environ['GOOGLE_API_KEY'])"
      ],
      "metadata": {
        "id": "_JFAzSdSs2g8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Removes the start and end of JSON codeblock formatting that the LLMs occasionally wrap JSON strings with.\n",
        "def strip_json_wrapper(json_str: str) -> str:\n",
        "    return json_str.replace('```json\\n', '').replace('\\n```', '')\n",
        "\n"
      ],
      "metadata": {
        "id": "U0SlA1kXszsl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Utility function to help with parsing prices from strings.\n",
        "def parse_price(price_str: str) -> float:\n",
        "    \"\"\"Extract numeric price value from the provided string.\"\"\"\n",
        "    if price_str == 'N/A':\n",
        "        return None\n",
        "\n",
        "    # Extract digits and decimal value\n",
        "    price_match = re.search(r'[\\d,.]+', price_str)\n",
        "    if not price_match:\n",
        "        return 0.0\n",
        "\n",
        "    # Remove commas and convert to float\n",
        "    price_digits = price_match.group(0).replace(',', '')\n",
        "    if price_digits and not price_digits.isspace():\n",
        "        return float(price_digits)\n",
        "\n"
      ],
      "metadata": {
        "id": "VXyfeoIrsu_b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Stateless tool - Search online for pre-built devices\n",
        "\n",
        "@tool\n",
        "def search_prebuilt(budget: str, requirements: list[str]) -> str:\n",
        "    \"\"\"Search for pre-built desktops or laptops fulfilling the user criteria.\n",
        "    Take the budget and requirements, and get Gemini to format it into a terse, concise query.\n",
        "    Then use search grounding to look for it and return the output as structured JSON with\n",
        "    price, brand, desktop/laptop, name, link to view more or buy\n",
        "    \"\"\"\n",
        "    print('CALLED TOOL: search_prebuilt')\n",
        "\n",
        "    newline_char = '\\n'\n",
        "    search_config = types.GenerateContentConfig(\n",
        "        tools=[types.Tool(google_search=types.GoogleSearch())],\n",
        "        temperature=0.0\n",
        "    )\n",
        "\n",
        "    prompt = f'''\n",
        "You are a computer hardware specialist who always responds in valid JSON. Search online\n",
        "for pre-built laptops or desktops based on the requirements provided below and return\n",
        "the results in JSON format with name, price, specifications, and purchase link for each\n",
        "device. Find at least 3-6 options for the device in question.\n",
        "\n",
        "JSON Structure to follow:\n",
        "Return a list of device objects, where each object has three fields: name, price, and specifications.\n",
        "\n",
        "Key points to note:\n",
        "- Please answer the following question using ONLY information found in the provided web search results. Cite your sources for each statement or paragraph.\n",
        "- Rely exclusively on the real-time search results to answer. For each device in the JSON list you make, indicate which search result supports it.\n",
        "\n",
        "Device Requirements:\n",
        "    1. Budget: {budget}\n",
        "    2. Requirements: {newline_char + (newline_char.join([((' ' * 8) + '- ' + req) for req in requirements]))}\n",
        "'''\n",
        "\n",
        "    response = None\n",
        "\n",
        "    response = client.models.generate_content(\n",
        "        model=MAIN_MODEL,\n",
        "        contents=prompt,\n",
        "        config=search_config\n",
        "    )\n",
        "    rc = response.candidates[0]\n",
        "    return rc.content.parts[0].text\n",
        "\n"
      ],
      "metadata": {
        "id": "c-4i5HMJsp3w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Stateless tool - Search hardware requirements for a particular task\n",
        "@tool\n",
        "def search_task_requirements(special_task_requirement: str) -> list[str]:\n",
        "    \"\"\"\n",
        "    Search hardware requirements for a particular task.\n",
        "    Takes in a string describing the task and returns a list of hardware specs.\n",
        "    \"\"\"\n",
        "    print('CALLED TOOL: search_task_requirements')\n",
        "    print('Special use case:', special_task_requirement)\n",
        "\n",
        "    sys_prompt = '''\n",
        "You are a computer hardware specialist. The user has some hardware intensive tasks they want\n",
        "to perform and they want to know the hardware specs their device will need for them to be able\n",
        "to do what they want. For gaming, it might be helpful to consider minimum and recommended specs.\n",
        "Search online for these specs needed and formulate that into a structured, comma-separated list\n",
        "of concise individual requirements. Respond ONLY WITH a concise list of requirements separated by\n",
        "commas, add no preface or conclusion.\n",
        "\n",
        "Key points to note:\n",
        "- Please answer the following question using ONLY information found in the provided web search results.\n",
        "- Rely exclusively on the real-time search results to answer. Do not provide anything that is not found online.\n",
        "\n",
        "Example answer 1:\n",
        "Windows 10/11 64-bit, AMD Ryzen 7 CPU 5700X CPU, 16GB RAM, AMD Radeon RX 6700 XT, 170 GB SSD\n",
        "\n",
        "Example answer 2:\n",
        "Windows 11 64-bit, 8GB RAM, Intel Core i7-10700 CPU, NVIDIA GeForce RTX 2080 (8GB VRAM) GPU\n",
        "\n",
        "Example answer 3:\n",
        "AMD Ryzen 5 3600 @ 3.6 GHz or Intel Core i7-8700K @ 3.7 GHz or better, 8 GB RAM, AMD RX 570 (4 GB) or NVIDIA GeForce GTX 1060 (6 GB) or better, 85.5 GB storage\n",
        "'''\n",
        "\n",
        "    response = client.models.generate_content(\n",
        "        model=MAIN_MODEL,\n",
        "        contents=f'The use case that the hardware is needed for: {special_task_requirement}',\n",
        "        config=types.GenerateContentConfig(\n",
        "            tools=[types.Tool(google_search=types.GoogleSearch())],\n",
        "            system_instruction=sys_prompt,\n",
        "            temperature=0.0\n",
        "        )\n",
        "    )\n",
        "\n",
        "    rc = response.candidates[0]\n",
        "    hardware_needed = rc.content.parts[0].text\n",
        "    if len(hardware_needed) > 0:\n",
        "        return hardware_needed.split(', ')\n",
        "    return hardware_needed"
      ],
      "metadata": {
        "id": "belP_iU8sj99"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Stateless tool - Lookup parts required internally using RAG and rules of thumb\n",
        "@tool\n",
        "def lookup_parts_needed(requirements: Iterable[str]) -> list[str]:\n",
        "    \"\"\"\n",
        "    Look to improve the suggested hardware from search_task_requirements using the rules outlined in system_prompt\n",
        "    :return list of components that will meet the requirements and conform to the rules\n",
        "    \"\"\"\n",
        "    print('CALLED TOOL: lookup_parts_needed')\n",
        "\n",
        "    # TODO: add everything to this file that's used ie query_db.py etc\n",
        "    # TODO: message kaggle forum about uploading embedding data and or txt files - @Gen AI Moderator - done\n",
        "    # TODO: look into pastebin if Kaggle no worky - done\n",
        "    # TODO: look into adding the embeddings for component installation tomorrow from human node\n",
        "\n",
        "    static_prompt = '''\n",
        "    You are a computer hardware specialist, able to provide PC hardware recommendations based off of\n",
        "    requirements you will be presented and using the provided text. When recommending parts you should conform to the\n",
        "    following rules:\n",
        "    RULES:\n",
        "    1. Select Graphics Processing Unit (GPU) that’s ~50% of the total budget\n",
        "    2. Select a Central Processing Unit (CPU) that does not bottleneck GPU\n",
        "    3. Select motherboard (MB) based on quality of above 2 components (budget vs pro build)\n",
        "    4. Select Random Access Memory (RAM) related to Overclock (OC) specs on CPU and MB\n",
        "         i. Don’t suggest RAM that has OC when MB/CPU does not support it, prioritize price in this situation\n",
        "        ii. Ensure RAM is on MB Qualified Vendors List (QVL)\n",
        "    5. Select a Power Supply Unit (PSU) that can support all components power draw in Watts (W)\n",
        "         i. Prioritize modular power supplies should budget allow\n",
        "        ii. Minimum 10% headroom over expected power draw to account for cooler/storage/fans etc\n",
        "       iii. Include optional 20-30% headroom for future upgrades if budget allows\n",
        "            a. Eg components expected power draw is 800W so suggest ~900W PSU\n",
        "    6. Selecting Storage\n",
        "         i. Prioritize traditional 2.5 Solid State Drives (SSDs) for budget builds with as much storage as possible\n",
        "        ii. Suggest M.2 SSDs for pro builds\n",
        "       iii. Hard Disk Drives (HDDs) only if minimum storage cannot be met within budget\n",
        "    7. Select a cooler that is both appropriate for the build and budget\n",
        "         i. eg not air cooler for hotter CPUs (Intel 14900K, etc.)\n",
        "    8. Select a case that will meet the GPU length requirements, MB type (ATX, ITX etc), and number of SSDs/HDDs suggested\n",
        "    9. Select fans depending on number of fans that come with the case\n",
        "         i. Aim to have minimum 3 (2 intake; 1 exhaust)\n",
        "        ii. Should budget allow, fill all fan slots on case\n",
        "    10. Any money left over from the budget should be used to improve the computer. For example, picking a better GPU,\n",
        "        CPU, more storage etc.\n",
        "\n",
        "    You must respond with a list of components, separated by commas with no additional information or comment. This list\n",
        "    must include a CPU, GPU, PSU, case, motherboard, fans, and storage unless the user has provided one of these already\n",
        "    as existing hardware.\n",
        "    '''\n",
        "\n",
        "    dynamic_prompt = '''\n",
        "    The parts recommended must be able to accommodate the following requirements and be supported by their passages:\n",
        "    '''\n",
        "\n",
        "    # Maybe try to get it in format Requirement: \\n Passage: (might use too many tokens)\n",
        "    for req in requirements:\n",
        "        static_prompt += f'REQUIREMENT: {req}\\n'\n",
        "        passages = []\n",
        "        passages += get_query_result(req)\n",
        "\n",
        "        for passage in passages:\n",
        "            passage_oneline = passage.replace('\\n', ' ')\n",
        "            static_prompt += f'PASSAGE: {passage_oneline}\\n'\n",
        "        print(static_prompt)\n",
        "\n",
        "    answer = client.models.generate_content(\n",
        "        model='gemini-2.0-flash',\n",
        "        contents=dynamic_prompt,\n",
        "        config=types.GenerateContentConfig(\n",
        "            system_instruction=static_prompt,\n",
        "            temperature=0.2\n",
        "        )\n",
        "    )\n",
        "    # print(answer)\n",
        "    answer = answer.text\n",
        "    answer = answer.replace('\\n', '')\n",
        "    print(answer)\n",
        "    return [s for s in answer.split(', ')]"
      ],
      "metadata": {
        "id": "wHsXXkS4sbpe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# All stateless tools, part of tools node: Search game specs, lookup parts needed (sends to\n",
        "# chatbot which then sends to search part then back to chatbot which then updates\n",
        "# recommendations; must also take in existing hardware), search custom part (searches\n",
        "# each part mentioned by lookup parts, takes in list of strs from chatbot or directly\n",
        "# from lookup, does google search or external api call,  returns json for each part) ->\n",
        "# this then goes to optimize build and then gets put in recommendations."
      ],
      "metadata": {
        "id": "xj2QranksY8h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Utility function\n",
        "def search_individual_part(budget: str, part_needed: str) -> tuple[str, Any]:\n",
        "    \"\"\"\n",
        "    \"\"\"\n",
        "    print(f'CALLED TOOL-HELPER: search_individual_part: {part_needed}')\n",
        "\n",
        "    newline_char = '\\n'\n",
        "\n",
        "    sys_prompt = '''\n",
        "You are a computer hardware specialist who always responds in valid JSON. Search online\n",
        "for computer hardware parts that match the part that the user is looking for. Be sure to\n",
        "bear the budget constraints in mind when looking. Recommend the latest parts that match\n",
        "this criteria and return the results in JSON format with name, price, specifications,\n",
        "and purchase link for each part. Find at least 2-4 options for this piece of hardware.\n",
        "\n",
        "JSON Structure to follow:\n",
        "Return a list of part objects, where each object has four fields: name, price, specifications,\n",
        "and purchase link.\n",
        "\n",
        "Key points to note:\n",
        "- You should focus your search to the US and assume that the budget is in USD ($).\n",
        "- Please answer the following question using ONLY information found in the provided web search results.\n",
        "- Rely exclusively on the real-time search results to answer.\n",
        "- Respond only in concise JSON as requested, do not add any preface or conclusions.\n",
        "'''\n",
        "\n",
        "    search_config = types.GenerateContentConfig(\n",
        "        tools=[types.Tool(google_search=types.GoogleSearch())],\n",
        "        temperature=0.0,\n",
        "        system_instruction=sys_prompt\n",
        "    )\n",
        "\n",
        "    response = client.models.generate_content(\n",
        "        model=MAIN_MODEL,\n",
        "        contents=f'Look for {part_needed} for a budget of {budget}.',\n",
        "        config=search_config\n",
        "    )\n",
        "    rc = response.candidates[0]\n",
        "    part_recommendations = rc.content.parts[0].text\n",
        "    part_recommendations = strip_json_wrapper(part_recommendations)\n",
        "    print(part_recommendations)\n",
        "\n",
        "    return (part_needed, json.loads(part_recommendations))"
      ],
      "metadata": {
        "id": "DDEYw7zDsToi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Parts scorer function\n",
        "def extract_specs_score(specs: str, part_type: str) -> float:\n",
        "    \"\"\"\n",
        "    Extract a numerical score from specifications based on part type.\n",
        "    Higher score means better performance.\n",
        "    \"\"\"\n",
        "    score = 0\n",
        "    specs_lower = specs.lower()\n",
        "\n",
        "    # CPU scoring\n",
        "    if any(cpu_term in part_type.lower() for cpu_term in ['cpu', 'processor', 'core']):\n",
        "        # Score based on cores\n",
        "        core_match = re.search(r'(\\d+)\\s*cores?', specs, re.IGNORECASE) or re.search(r'(\\d+)[\\s-]*core', specs, re.IGNORECASE)\n",
        "        if core_match:\n",
        "            score += int(core_match.group(1)) * 10\n",
        "\n",
        "        # Score based on threads\n",
        "        thread_match = re.search(r'(\\d+)\\s*threads?', specs, re.IGNORECASE)\n",
        "        if thread_match:\n",
        "            score += int(thread_match.group(1)) * 5\n",
        "\n",
        "        # Score based on clock speed\n",
        "        clock_match = re.search(r'(\\d+\\.\\d+)\\s*GHz', specs, re.IGNORECASE)\n",
        "        if clock_match:\n",
        "            score += float(clock_match.group(1)) * 20\n",
        "\n",
        "        # Score based on cache\n",
        "        cache_match = re.search(r'(\\d+)\\s*MB\\s*[Cc]ache', specs)\n",
        "        if cache_match:\n",
        "            score += int(cache_match.group(1)) * 2\n",
        "\n",
        "        # Bonus for newer generations\n",
        "        if '14' in specs:\n",
        "            score += 50\n",
        "        elif '13' in specs:\n",
        "            score += 40\n",
        "        elif '12' in specs:\n",
        "            score += 30\n",
        "        elif '11' in specs:\n",
        "            score += 20\n",
        "        elif '10' in specs:\n",
        "            score += 10\n",
        "\n",
        "    # GPU scoring\n",
        "    elif any(gpu_term in part_type.lower() for gpu_term in ['gpu', 'graphics', 'video card', 'geforce', 'radeon']):\n",
        "        # Score based on CUDA cores or Stream Processors\n",
        "        cuda_match = re.search(r'(\\d+,?\\d*)\\s*CUDA', specs, re.IGNORECASE)\n",
        "        if cuda_match:\n",
        "            cuda_cores = int(cuda_match.group(1).replace(',', ''))\n",
        "            score += cuda_cores / 100\n",
        "\n",
        "        stream_match = re.search(r'(\\d+,?\\d*)\\s*Stream\\s*Processors', specs, re.IGNORECASE)\n",
        "        if stream_match:\n",
        "            stream_processors = int(stream_match.group(1).replace(',', ''))\n",
        "            score += stream_processors / 80\n",
        "\n",
        "        # Score based on memory\n",
        "        memory_match = re.search(r'(\\d+)\\s*GB', specs, re.IGNORECASE)\n",
        "        if memory_match:\n",
        "            score += int(memory_match.group(1)) * 10\n",
        "\n",
        "        # Score based on memory speed\n",
        "        speed_match = re.search(r'(\\d+\\.\\d+)\\s*Gbps', specs, re.IGNORECASE)\n",
        "        if speed_match:\n",
        "            score += float(speed_match.group(1)) * 2\n",
        "\n",
        "        # Score based on memory bus width\n",
        "        bus_match = re.search(r'(\\d+)[\\s-]*bit', specs, re.IGNORECASE)\n",
        "        if bus_match:\n",
        "            score += int(bus_match.group(1)) / 10\n",
        "\n",
        "        # Score based on clock speeds\n",
        "        clock_match = re.search(r'(\\d+)\\s*MHz', specs, re.IGNORECASE)\n",
        "        if clock_match:\n",
        "            score += int(clock_match.group(1)) / 100\n",
        "\n",
        "    # RAM scoring\n",
        "    elif any(ram_term in part_type.lower() for ram_term in ['ram', 'memory', 'ddr']):\n",
        "        # Score based on capacity\n",
        "        capacity_match = re.search(r'(\\d+)\\s*GB', specs, re.IGNORECASE)\n",
        "        if capacity_match:\n",
        "            score += int(capacity_match.group(1)) * 10\n",
        "\n",
        "        # Score based on speed\n",
        "        speed_match = re.search(r'(\\d+)\\s*MHz', specs, re.IGNORECASE) or re.search(r'DDR\\d+-(\\d+)', specs, re.IGNORECASE)\n",
        "        if speed_match:\n",
        "            score += int(speed_match.group(1)) / 100\n",
        "\n",
        "        # Score based on CAS latency\n",
        "        cas_match = re.search(r'CL(\\d+)', specs, re.IGNORECASE)\n",
        "        if cas_match:\n",
        "            # Lower CAS is better, so we invert the relationship\n",
        "            score += 20 - int(cas_match.group(1))\n",
        "\n",
        "        # Type of RAM bonus\n",
        "        if 'ddr5' in specs_lower:\n",
        "            score += 50\n",
        "        elif 'ddr4' in specs_lower:\n",
        "            score += 30\n",
        "        elif 'ddr3' in specs_lower:\n",
        "            score += 10\n",
        "\n",
        "    # Storage scoring\n",
        "    elif any(storage_term in part_type.lower() for storage_term in ['ssd', 'hdd', 'storage', 'drive', 'nvme']):\n",
        "        # Score based on capacity\n",
        "        tb_match = re.search(r'(\\d+)\\s*TB', specs, re.IGNORECASE)\n",
        "        if tb_match:\n",
        "            score += int(tb_match.group(1)) * 100\n",
        "\n",
        "        gb_match = re.search(r'(\\d+)\\s*GB', specs, re.IGNORECASE)\n",
        "        if gb_match:\n",
        "            score += int(gb_match.group(1)) / 10\n",
        "\n",
        "        # Score based on read/write speeds\n",
        "        read_match = re.search(r'(\\d+,?\\d*)\\s*MB\\/s read', specs, re.IGNORECASE) or re.search(r'read:?\\s*(\\d+,?\\d*)\\s*MB\\/s', specs, re.IGNORECASE)\n",
        "        if read_match:\n",
        "            score += int(read_match.group(1).replace(',', '')) / 100\n",
        "\n",
        "        # Type bonus\n",
        "        if any(fast_storage in specs_lower for fast_storage in ['nvme', 'm.2', 'pcie 4.0', 'pcie4']):\n",
        "            score += 50\n",
        "        elif 'ssd' in specs_lower:\n",
        "            score += 30\n",
        "        elif 'hdd' in specs_lower:\n",
        "            score += 10\n",
        "\n",
        "    # Motherboard scoring\n",
        "    elif any(mb_term in part_type.lower() for mb_term in ['motherboard', 'mainboard', 'mobo']):\n",
        "        # Score based on chipset/generation\n",
        "        chipset_score = 0\n",
        "        if any(x in specs_lower for x in ['z790', 'x670', 'x670e']):\n",
        "            chipset_score = 50\n",
        "        elif any(x in specs_lower for x in ['z690', 'x570', 'b650']):\n",
        "            chipset_score = 40\n",
        "        elif any(x in specs_lower for x in ['b550', 'z590', 'b560']):\n",
        "            chipset_score = 30\n",
        "        score += chipset_score\n",
        "\n",
        "        # Score based on memory support\n",
        "        if 'ddr5' in specs_lower:\n",
        "            score += 30\n",
        "        elif 'ddr4' in specs_lower:\n",
        "            score += 15\n",
        "\n",
        "        # Score based on PCIe support\n",
        "        if 'pcie 5.0' in specs_lower or 'pcie5' in specs_lower:\n",
        "            score += 30\n",
        "        elif 'pcie 4.0' in specs_lower or 'pcie4' in specs_lower:\n",
        "            score += 20\n",
        "        elif 'pcie 3.0' in specs_lower or 'pcie3' in specs_lower:\n",
        "            score += 10\n",
        "\n",
        "        # Score based on connectivity\n",
        "        if 'wifi' in specs_lower:\n",
        "            score += 15\n",
        "        if 'bluetooth' in specs_lower:\n",
        "            score += 10\n",
        "        if 'usb 3' in specs_lower or 'usb-c' in specs_lower:\n",
        "            score += 15\n",
        "\n",
        "    # PSU scoring\n",
        "    elif any(psu_term in part_type.lower() for psu_term in ['psu', 'power supply', 'power']):\n",
        "        # Score based on wattage\n",
        "        wattage_match = re.search(r'(\\d+)\\s*W', specs, re.IGNORECASE) or re.search(r'(\\d+)\\s*watt', specs, re.IGNORECASE)\n",
        "        if wattage_match:\n",
        "            score += int(wattage_match.group(1)) / 10\n",
        "\n",
        "        # Score based on efficiency\n",
        "        if '80+ titanium' in specs_lower:\n",
        "            score += 50\n",
        "        elif '80+ platinum' in specs_lower:\n",
        "            score += 40\n",
        "        elif '80+ gold' in specs_lower:\n",
        "            score += 30\n",
        "        elif '80+ silver' in specs_lower:\n",
        "            score += 20\n",
        "        elif '80+ bronze' in specs_lower:\n",
        "            score += 10\n",
        "\n",
        "        # Score based on modularity\n",
        "        if 'full modular' in specs_lower or 'fully modular' in specs_lower:\n",
        "            score += 20\n",
        "        elif 'semi modular' in specs_lower or 'semi-modular' in specs_lower:\n",
        "            score += 10\n",
        "\n",
        "    # Case scoring\n",
        "    elif any(case_term in part_type.lower() for case_term in ['case', 'chassis', 'tower']):\n",
        "        # Score based on form factor\n",
        "        if 'full tower' in specs_lower:\n",
        "            score += 30\n",
        "        elif 'mid tower' in specs_lower:\n",
        "            score += 20\n",
        "        elif 'mini' in specs_lower:\n",
        "            score += 10\n",
        "\n",
        "        # Score based on features\n",
        "        if 'tempered glass' in specs_lower:\n",
        "            score += 15\n",
        "        if 'rgb' in specs_lower:\n",
        "            score += 10\n",
        "        if 'usb-c' in specs_lower or 'usb 3' in specs_lower:\n",
        "            score += 15\n",
        "\n",
        "        # Score based on cooling support\n",
        "        fan_match = re.search(r'(\\d+)\\s*fans?', specs, re.IGNORECASE)\n",
        "        if fan_match:\n",
        "            score += int(fan_match.group(1)) * 5\n",
        "\n",
        "    # Cooling scoring\n",
        "    elif any(cooling_term in part_type.lower() for cooling_term in ['fan', 'cooler', 'cooling', 'aio']):\n",
        "        # Score based on size\n",
        "        mm_match = re.search(r'(\\d+)\\s*mm', specs, re.IGNORECASE)\n",
        "        if mm_match:\n",
        "            score += int(mm_match.group(1)) / 10\n",
        "\n",
        "        # Score based on type\n",
        "        if 'aio' in specs_lower or 'liquid' in specs_lower or 'water' in specs_lower:\n",
        "            score += 30\n",
        "        elif 'air' in specs_lower:\n",
        "            score += 15\n",
        "\n",
        "        # Score based on RGB\n",
        "        if 'rgb' in specs_lower:\n",
        "            score += 10\n",
        "\n",
        "    # Generic scoring for all part types based on specs length\n",
        "    # If specific metrics aren't found, we'll assume more text means more features\n",
        "    score += len(specs) * 0.01\n",
        "\n",
        "    return score"
      ],
      "metadata": {
        "id": "nh-qhV9NsRgT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def rank_options_available(parts_data: Dict[str, List[Dict[str, str]]], budget: float) -> List[Dict[str, Any]]:\n",
        "    \"\"\"\n",
        "    Rank parts based on specifications and price, within budget.\n",
        "    Returns a list of top three ranked parts with calculated metrics.\n",
        "    \"\"\"\n",
        "    all_parts = []\n",
        "\n",
        "    for part_type, parts in parts_data.items():\n",
        "        for part in parts:\n",
        "            price = parse_price(part['price'])\n",
        "            if price is None:\n",
        "                continue  # Skip parts with no price\n",
        "\n",
        "            if price > budget:\n",
        "                continue  # Skip parts over budget\n",
        "\n",
        "            specs_score = extract_specs_score(part['specifications'], part_type)\n",
        "\n",
        "            # Calculate value score (performance per dollar)\n",
        "            value_score = specs_score / price if price > 0 else 0\n",
        "\n",
        "            all_parts.append({\n",
        "                'name': part['name'],\n",
        "                'price': part['price'],\n",
        "                'raw_price': price,\n",
        "                'specifications': part['specifications'],\n",
        "                'purchase link': part['purchase link'],\n",
        "                'part_type': part_type,\n",
        "                'specs_score': specs_score,\n",
        "                'value_score': value_score\n",
        "            })\n",
        "\n",
        "    # Sort by specs score first (higher is better)\n",
        "    all_parts.sort(key=lambda x: x['specs_score'], reverse=True)\n",
        "\n",
        "    # Get top three parts\n",
        "    top_parts = all_parts[:3]\n",
        "\n",
        "    # Clean up the output by removing temporary fields\n",
        "    for part in top_parts:\n",
        "        part.pop('raw_price', None)\n",
        "        part.pop('specs_score', None)\n",
        "        part.pop('value_score', None)\n",
        "\n",
        "    return top_parts"
      ],
      "metadata": {
        "id": "Y_4ovg9msF61"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def process_parts(json_data: str, budget: float) -> str:\n",
        "    \"\"\"\n",
        "    Main function to process parts data and return top three ranked parts.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Parse JSON string to dictionary\n",
        "        parts_dict = json.loads(json_data)\n",
        "\n",
        "        # Rank parts\n",
        "        top_parts = {}\n",
        "        for part_type, part_data in parts_dict.items():\n",
        "            top_parts[part_type] = rank_options_available({part_type: part_data}, budget)\n",
        "\n",
        "        # Convert results back to JSON\n",
        "        result_json = json.dumps(top_parts, indent=4, ensure_ascii=False)\n",
        "        return result_json\n",
        "\n",
        "    except json.JSONDecodeError:\n",
        "        return json.dumps({'error': 'Invalid JSON input'})\n",
        "    except Exception as e:\n",
        "        return json.dumps({'error': str(e)})"
      ],
      "metadata": {
        "id": "ULVUPydksCsp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Stateless tool - Search online for custom parts for the build\n",
        "@tool\n",
        "def search_custom_parts(budget: str, parts_needed: Iterable[str] = []) -> Dict[str, list[Dict[str, Any]]]:\n",
        "    \"\"\"\n",
        "    \"\"\"\n",
        "    print('CALLED TOOL: search_custom_parts')\n",
        "\n",
        "    parts_found = {}\n",
        "    for part_needed in parts_needed:\n",
        "        part_type, part_data = search_individual_part(budget, part_needed)\n",
        "        parts_found[part_type] = part_data\n",
        "\n",
        "    return json.dumps(parts_found, indent=4, ensure_ascii=False)"
      ],
      "metadata": {
        "id": "6vF8Chj6sA-v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tool signatures for planning the build\n",
        "# Functionality defined in pc_planner_node\n",
        "@tool\n",
        "def update_plans(requirements: Iterable[str], budget: Optional[str] = None) -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Adds or modifies the device requirements and budget.\n",
        "    Returns a confirmation of the budget and requirements that were just added to state.\n",
        "    \"\"\""
      ],
      "metadata": {
        "id": "5G6JJEY3r-H3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@tool\n",
        "def clear_plan():\n",
        "    \"\"\"\n",
        "    Removes all requirements and budget information and resets to blank slate.\n",
        "    \"\"\""
      ],
      "metadata": {
        "id": "4ipW1n_4r8DF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tool signatures for recommending parts for a planned build\n",
        "# Functionality defined in optimize_build_node\n",
        "@tool\n",
        "def rank_parts(recommended_parts: str = '') -> str:\n",
        "    \"\"\"\n",
        "    Takes in a list of parts in JSON and a budget and returns the most\n",
        "    performant yet price-optimal parts for each part.\n",
        "    Modifies state by adding the parts to the recommendations list.\n",
        "    \"\"\""
      ],
      "metadata": {
        "id": "EZKZ-vkTr6rS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@tool\n",
        "def rank_prebuilds(recommended_devices: str = '') -> str:\n",
        "    \"\"\"\n",
        "    Takes in a list of prebuilt devices in JSON and a budget and returns the top\n",
        "    three most performant yet price-optimal devices, also in JSON.\n",
        "    Modifies state by adding the devices to the recommendations list.\n",
        "    \"\"\"\n",
        "\n",
        "# Helper functions required to rank pre-built devices\n",
        "\n",
        "def score_specs(specs: str) -> int:\n",
        "    \"\"\"\n",
        "    Analyze specifications to score device performance.\n",
        "    Higher scores indicate better performance.\n",
        "    \"\"\"\n",
        "    score = 0\n",
        "    specs = specs.lower()\n",
        "\n",
        "    # CPU scoring\n",
        "    if 'ryzen 7' in specs or 'i7' in specs:\n",
        "        score += 80\n",
        "    elif 'ryzen 5' in specs or 'i5' in specs:\n",
        "        score += 60\n",
        "    elif 'ryzen 3' in specs or 'i3' in specs:\n",
        "        score += 40\n",
        "\n",
        "    # Generation bonus\n",
        "    if '5700' in specs:\n",
        "        score += 20\n",
        "    elif '5600' in specs or '5500' in specs:\n",
        "        score += 15\n",
        "    elif '4500' in specs or '4600' in specs:\n",
        "        score += 10\n",
        "\n",
        "    # RAM scoring\n",
        "    ram_match = re.search(r'(\\d+)gb', specs.replace(' ', ''))\n",
        "    if ram_match:\n",
        "        ram_size = int(ram_match.group(1))\n",
        "        if ram_size >= 32:\n",
        "            score += 50\n",
        "        elif ram_size >= 16:\n",
        "            score += 30\n",
        "        elif ram_size >= 8:\n",
        "            score += 15\n",
        "\n",
        "    # Storage scoring\n",
        "    if '1tb' in specs.replace(' ', ''):\n",
        "        score += 30\n",
        "    elif '500gb' in specs.replace(' ', '') or '512gb' in specs.replace(' ', ''):\n",
        "        score += 20\n",
        "\n",
        "    if 'nvme' in specs or 'ssd' in specs:\n",
        "        score += 20\n",
        "\n",
        "    # GPU scoring\n",
        "    if 'rtx 3080' in specs:\n",
        "        score += 100\n",
        "    elif 'rtx 3070' in specs:\n",
        "        score += 80\n",
        "    elif 'rtx 3060' in specs:\n",
        "        score += 70\n",
        "    elif 'gtx 1650' in specs:\n",
        "        score += 40\n",
        "    elif 'radeon' in specs or 'onboard' in specs:\n",
        "        score += 20\n",
        "\n",
        "    return score"
      ],
      "metadata": {
        "id": "bPjnDZDBr4tU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_value_ratio(device: Dict[str, Any], budget: float) -> float:\n",
        "    \"\"\"\n",
        "    Calculate value ratio based on specs score and price.\n",
        "    Returns 0 if device exceeds budget.\n",
        "    \"\"\"\n",
        "    price = parse_price(device['price'])\n",
        "    if price > budget:\n",
        "        return 0\n",
        "\n",
        "    specs_score = score_specs(device['specifications'])\n",
        "\n",
        "    # Calculate value ratio (specs score per unit of price)\n",
        "    # Higher ratio means better value\n",
        "    if price > 0:\n",
        "        return specs_score / price\n",
        "    return 0"
      ],
      "metadata": {
        "id": "vCcTvL_Pr2aO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def rank_devices(devices_json: str, budget: float, return_json: Optional[bool] = False) -> Any:\n",
        "    \"\"\"\n",
        "    Rank devices based on specifications and price within budget.\n",
        "    Returns JSON string with top 3 devices.\n",
        "    \"\"\"\n",
        "    devices = json.loads(devices_json)\n",
        "    if isinstance(devices, dict):\n",
        "        devices = devices['devices']\n",
        "\n",
        "    for device in devices:\n",
        "        device['value_ratio'] = get_value_ratio(device, budget)\n",
        "\n",
        "    # Sort devices by assigned value ratios descending\n",
        "    ranked_devices = sorted(devices, key=lambda x: x['value_ratio'], reverse=True)\n",
        "\n",
        "    # Select top 3 within budget\n",
        "    top_devices = [\n",
        "        {k: v for k, v in device.items() if k != 'value_ratio'}\n",
        "        for device in ranked_devices[:3] if parse_price(device['price']) <= budget\n",
        "    ]\n",
        "\n",
        "    if return_json:\n",
        "        return json.dumps(top_devices, indent=4, ensure_ascii=False)\n",
        "    return top_devices"
      ],
      "metadata": {
        "id": "9KVwQ2-Qr0tB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tool grouped on nodes\n",
        "planner_tools = [update_plans, clear_plan]\n",
        "builder_tools = [rank_parts, rank_prebuilds]\n",
        "\n",
        "# Tools Config\n",
        "auto_tools = [search_prebuilt, search_task_requirements, lookup_parts_needed, search_custom_parts]\n",
        "tool_node = ToolNode(auto_tools)\n",
        "\n",
        "# Tool binding\n",
        "llm_with_tools = llm.bind_tools(auto_tools + planner_tools + builder_tools)"
      ],
      "metadata": {
        "id": "6uu9zEZ5rzCQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Build planner node\n",
        "def pc_planner_node(state: PCBuilderState) -> PCBuilderState:\n",
        "    \"\"\"This is where the requirements and budget within state get manipulated.\"\"\"\n",
        "\n",
        "    tool_msg = state.get(\"messages\", [])[-1]\n",
        "    requirements_state = state.get(\"requirements\", [])\n",
        "    budget_state = state.get('budget', None)\n",
        "    recommendations_state = state.get('recommendations', [])\n",
        "    outbound_msgs = []\n",
        "    build_complete = state.get('build_complete', False)\n",
        "\n",
        "    for tool_call in tool_msg.tool_calls:\n",
        "        if tool_call['name'] == 'update_plans':\n",
        "            print('CALLED TOOL: update_plans')\n",
        "            args_given = tool_call['args']\n",
        "            requirements_arg = budget_arg = None\n",
        "            if 'requirements' in args_given:\n",
        "                requirements_arg = args_given['requirements']\n",
        "            if 'budget' in args_given:\n",
        "                budget_arg = args_given['budget']\n",
        "            # If budget is None and nothing exists in state, raise an error.\n",
        "            # If there is something in state, then just don't update it.\n",
        "            # Otherwise, always update the budget.\n",
        "            if budget_arg is None or len(budget_arg) < 1:\n",
        "                if budget_state is None or len(budget_state) < 1:\n",
        "                    raise ValueError('Budget is missing in tool call as well as state!')\n",
        "            else:\n",
        "                budget_state = budget_arg\n",
        "            if requirements_arg is None or len(requirements_arg) < 1:\n",
        "                if requirements_state is None or len(requirements_state) < 1:\n",
        "                    raise ValueError('Requirements are missing in tool call as well as state!')\n",
        "            else:\n",
        "                requirements_state = [requirement for requirement in requirements_arg]\n",
        "            response = {\n",
        "                'budget': budget_arg if budget_arg is not None else budget_state,\n",
        "                'requirements': requirements_arg\n",
        "            }\n",
        "\n",
        "        elif tool_call['name'] == 'clear_plan':\n",
        "            print('CALLED TOOL: clear_plan')\n",
        "            requirements_state = []\n",
        "            budget_state = None\n",
        "            response = None\n",
        "\n",
        "        else:\n",
        "            raise NotImplementedError(f'Unknown tool call: {tool_call[\"name\"]}')\n",
        "\n",
        "        # Record the tool results as tool messages.\n",
        "        outbound_msgs.append(\n",
        "            ToolMessage(\n",
        "                content=response,\n",
        "                name=tool_call[\"name\"],\n",
        "                tool_call_id=tool_call[\"id\"],\n",
        "            )\n",
        "        )\n",
        "\n",
        "    return {\n",
        "        'messages': outbound_msgs,\n",
        "        'requirements': requirements_state,\n",
        "        'budget': budget_state,\n",
        "        'recommendations': recommendations_state,\n",
        "        'build_complete': build_complete\n",
        "    }"
      ],
      "metadata": {
        "id": "zvN426Ztrw4k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Parts recommender node (based on devised plan and recommended devices)\n",
        "def optimize_build_node(state: PCBuilderState) -> PCBuilderState:\n",
        "    \"\"\"This is where the recommendations within state get manipulated.\"\"\"\n",
        "    # state.recommendations modified by tool rank_parts() part of optimize_build node\n",
        "    # - happens once search_prebuilt() tool returns a few values\n",
        "\n",
        "    tool_msg = state.get('messages', [])[-1]\n",
        "    requirements_state = state.get('requirements', [])\n",
        "    budget_state = state.get('budget', None)\n",
        "    recommendations_state = state.get('recommendations', [])\n",
        "    outbound_msgs = []\n",
        "    build_complete = state.get('build_complete', False)\n",
        "\n",
        "    for tool_call in tool_msg.tool_calls:\n",
        "        args_available = tool_call['args']\n",
        "        if tool_call['name'] == 'rank_parts':\n",
        "            print('CALLED TOOL: rank_parts')\n",
        "            if 'recommended_parts' not in args_available:\n",
        "                raise ValueError('There are no recommended parts available to rank!')\n",
        "            recommended_parts = args_available['recommended_parts']\n",
        "            if budget_state is None or len(budget_state) < 1:\n",
        "                raise ValueError('An invalid budget was found within state!')\n",
        "            print('\\nThis is the custom parts recommendations json provided:\\n', recommended_parts, '\\n')\n",
        "            recommendations_state = process_parts(strip_json_wrapper(recommended_parts), parse_price(budget_state))\n",
        "            response = recommendations_state\n",
        "\n",
        "        elif tool_call['name'] == 'rank_prebuilds':\n",
        "            print('CALLED TOOL: rank_prebuilds')\n",
        "            recommended_devices = tool_call['args']['recommended_devices']\n",
        "            if budget_state is None or len(budget_state) < 1:\n",
        "                raise ValueError('An invalid budget was found within state!')\n",
        "            print('\\nThis is the prebuilds recommendations json provided:\\n', recommended_devices, '\\n')\n",
        "            recommendations_state = rank_devices(recommended_devices, parse_price(budget_state))\n",
        "            response = rank_devices(recommended_devices, parse_price(budget_state), True)\n",
        "\n",
        "        else:\n",
        "            raise NotImplementedError(f'Unknown tool call: {tool_call[\"name\"]}')\n",
        "\n",
        "        # Record the tool results as tool messages.\n",
        "        outbound_msgs.append(\n",
        "            ToolMessage(\n",
        "                content=response,\n",
        "                name=tool_call['name'],\n",
        "                tool_call_id=tool_call['id'],\n",
        "            )\n",
        "        )\n",
        "\n",
        "    return {\n",
        "        'messages': outbound_msgs,\n",
        "        'requirements': requirements_state,\n",
        "        'budget': budget_state,\n",
        "        'recommendations': recommendations_state,\n",
        "        'build_complete': build_complete\n",
        "    }"
      ],
      "metadata": {
        "id": "RBOmcC8prtw1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def chatbot_node(state: PCBuilderState) -> PCBuilderState:\n",
        "    \"\"\"The chatbot itself. A simple wrapper around the model's own chat interface.\"\"\"\n",
        "    default_state = {'requirements': [], 'budget': None, 'recommendations': [], 'build_complete': False}\n",
        "\n",
        "    if state['messages']:\n",
        "        # If there are messages, continue the conversation with the model\n",
        "        message_history = [PC_BUILDER_SYSINT] + state['messages']\n",
        "        new_output = llm_with_tools.invoke(message_history)\n",
        "    else:\n",
        "        # If there are no messages, welcome the user.\n",
        "        new_output = AIMessage(content=WELCOME_MSG)\n",
        "\n",
        "    # Setup some defaults, then override with whatever exists in state, and finally\n",
        "    # override with messages\n",
        "    return default_state | state | {'messages': [new_output]}"
      ],
      "metadata": {
        "id": "qeweX96Vro72"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def human_node(state: PCBuilderState) -> PCBuilderState:\n",
        "    \"\"\"Display the last message from the model to the user, and receive their input.\"\"\"\n",
        "\n",
        "    print('\\nAll state until now:')\n",
        "    for key, val in state.items():\n",
        "        print(key, '::', val)\n",
        "        print('\\n')\n",
        "    print('\\nNow onto the messages:\\n')\n",
        "\n",
        "    last_msg = state['messages'][-1]\n",
        "    print('Model:', last_msg.content)\n",
        "\n",
        "    user_input = input('User: ')\n",
        "\n",
        "    # Does the user wish to quit?\n",
        "    if user_input in {'q', 'quit', 'exit', 'goodbye'}:\n",
        "        state['build_complete'] = True\n",
        "\n",
        "    return state | {'messages': [('user', user_input)]}\n",
        "\n",
        "# Human to Exit OR Human to Chatbot; Conditional Edge Transition function\n",
        "def maybe_exit_human_node(state: PCBuilderState) -> Literal[\"chatbot\", \"__end__\"]:\n",
        "    \"\"\"Route to the chatbot, unless it looks like the user is exiting.\"\"\"\n",
        "    if state.get('build_complete', False):\n",
        "        return END\n",
        "    else:\n",
        "        return 'chatbot'"
      ],
      "metadata": {
        "id": "2GmuQtarrmyr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Chatbot to Tools OR Chatbot to Human; Conditional Edge Transition function\n",
        "def maybe_route_to_tools(state: PCBuilderState) -> str:\n",
        "    if not (msgs := state.get('messages', [])):\n",
        "        raise ValueError(f'No messages found when parsing state: {state}')\n",
        "\n",
        "    # Only route based on the last message.\n",
        "    msg = msgs[-1]\n",
        "\n",
        "    if state.get('build_complete', False):\n",
        "        # If the user has no more questions or indicates satisfaction, complete the build\n",
        "        return END\n",
        "    elif hasattr(msg, 'tool_calls') and len(msg.tool_calls) > 0:\n",
        "        # When chatbot returns tool_calls, route to the 'tools' node\n",
        "        if any(tool['name'] in tool_node.tools_by_name.keys() for tool in msg.tool_calls):\n",
        "            return 'tools'\n",
        "        elif any(tool['name'] in [func.name for func in planner_tools] for tool in msg.tool_calls):\n",
        "            return 'pc_planner'\n",
        "        elif any(tool['name'] in [func.name for func in builder_tools] for tool in msg.tool_calls):\n",
        "            return 'optimize_build'\n",
        "        else:\n",
        "            raise ValueError('A nonexistent node was called.')\n",
        "    else:\n",
        "        return 'human'"
      ],
      "metadata": {
        "id": "WQNYJ8EfrlDh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set up the initial graph based on our state definition.\n",
        "graph_builder = StateGraph(PCBuilderState)\n",
        "\n",
        "# Add all the nodes to the app graph.\n",
        "graph_builder.add_node('chatbot', chatbot_node)\n",
        "graph_builder.add_node('human', human_node)\n",
        "graph_builder.add_node('tools', tool_node)\n",
        "graph_builder.add_node('pc_planner', pc_planner_node)\n",
        "graph_builder.add_node('optimize_build', optimize_build_node)\n",
        "\n",
        "# Define the chatbot node as the app entrypoint.\n",
        "graph_builder.add_edge(START, 'chatbot')\n",
        "\n",
        "# Edge transitions\n",
        "graph_builder.add_conditional_edges('chatbot', maybe_route_to_tools)\n",
        "graph_builder.add_conditional_edges('human', maybe_exit_human_node)\n",
        "graph_builder.add_edge('tools', 'chatbot')\n",
        "graph_builder.add_edge('pc_planner', 'chatbot')\n",
        "graph_builder.add_edge('optimize_build', 'chatbot')\n",
        "\n",
        "chat_graph = graph_builder.compile()"
      ],
      "metadata": {
        "id": "r0SEJjYcrjN9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualize the graph created.\n",
        "Image(chat_graph.get_graph().draw_mermaid_png())"
      ],
      "metadata": {
        "id": "WS_FIYVmrgMh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# The default recursion limit for traversing nodes is 25 - setting it higher means\n",
        "# you can try a more complex order with multiple steps and round-trips (and you\n",
        "# can chat for longer!)\n",
        "config = {'recursion_limit': 100}\n",
        "\n",
        "# Remember that this will loop forever, unless you input `q`, `quit` or one of the\n",
        "# other exit terms defined in `human_node`.\n",
        "# Uncomment this line to execute the graph:\n",
        "state = chat_graph.invoke({'messages': []}, config)\n",
        "\n",
        "# Things to try:\n",
        "#  - Just chat! There's no ordering or menu yet.\n",
        "#  - 'q' to exit.\n",
        "\n",
        "pprint(state)"
      ],
      "metadata": {
        "id": "E7aalmYgreld"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}