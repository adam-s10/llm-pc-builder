{
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3",
      "language": "python"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.11",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "colab": {
      "provenance": []
    },
    "kaggle": {
      "accelerator": "none",
      "dataSources": [
        {
          "sourceId": 11490863,
          "sourceType": "datasetVersion",
          "datasetId": 7203008
        }
      ],
      "dockerImageVersionId": 31012,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": false
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# ðŸ§  Build Your Dream Computer: A Generative AI Assistant for Planning PC Builds ðŸ’»âœ¨\n",
        "\n",
        "Welcome to the **PC Builder Assistant** notebook â€” your AI-powered companion for navigating the complex world of custom and pre-built PCs. Whether you're a gamer, content creator, office user, or machine learning enthusiast, this assistant is designed to help you discover the **best machine for your needs and your budget**.\n",
        "\n",
        "[![PCBuilderBot](https://i.imgur.com/NY3g3FM.png)](https://imgur.com/a/pc-builder-bot-EMvbcxL)\n",
        "\n",
        "Made as the final Capstone Project submission for the [Google x Kaggle Generative AI Hackathon](https://www.kaggle.com/competitions/gen-ai-intensive-course-capstone-2025q1) held in April 2025."
      ],
      "metadata": {
        "id": "Z1DKra13Qyl8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ðŸ’¡ The Problem\n",
        "\n",
        "Choosing the right computer â€” whether it's a prebuilt device or a custom-built rig â€” is **overwhelming**. You need to consider use cases, compatible components, budget constraints, power draw, hardware bottlenecks, and a sea of ever-changing product options.\n",
        "\n",
        "This is especially hard for non-technical users, or those who donâ€™t keep up with the latest PC hardware trends.\n",
        "\n",
        "---\n",
        "\n",
        "## ðŸ§™â€â™‚ï¸ Enter Generative AI\n",
        "\n",
        "Weâ€™ve built a **generative AI-powered agent** that acts like a **knowledgeable PC-building expert**, always available to:\n",
        "\n",
        "- Ask you the right questions about your needs ðŸ—£ï¸  \n",
        "- Understand and organize your requirements ðŸ“‹  \n",
        "- Search real-time data from the web ðŸ”  \n",
        "- Score and rank PC components or prebuilt options based on performance and price ðŸ†  \n",
        "- Return **final build recommendations** with clear justification ðŸ“¦  \n",
        "\n",
        "This assistant is built using **LangGraph** to model complex multi-step workflows, and uses **Google Gemini** (via LangChain) as its brain. Combined with **ChromaDB** for knowledge retrieval and custom scoring algorithms for hardware, it makes planning your new PC rig a seamless, intuitive experience.\n",
        "\n",
        "### ðŸ”§ Generative AI Capabilities Used in This Project\n",
        "\n",
        "This project leverages a variety of powerful Generative AI capabilities to make the PC Builder Assistant intelligent, context-aware, and action-oriented:\n",
        "\n",
        "- ðŸ§± **Structured Output / JSON Generation**  \n",
        "  The Gemini model is prompt-engineered to always return clean, parseable JSON â€” this enables seamless communication between AI and tools.\n",
        "\n",
        "- ðŸ§  **Few-Shot Prompting**  \n",
        "  The assistant is primed with examples and guided logic when searching for certain task requirements, ensuring it behaves consistently and intelligently.\n",
        "\n",
        "- ðŸ› ï¸ **Function Calling (Tool Use)**  \n",
        "  The LLM can call external functions (`tools`) to take specific actions like searching for parts or updating the user's plan. These include both *stateful (developed-controlled)* and *stateless (automatically executed)* tools.\n",
        "\n",
        "- ðŸ¤– **Agentic Workflow**  \n",
        "  The assistant functions as an agent within a LangGraph workflow, dynamically deciding what to do based on context and past state.\n",
        "\n",
        "- ðŸ“¡ **Grounding with Real-Time Search**  \n",
        "  When needed, the assistant uses the Gemini + Google Search tool to fetch fresh data about prices, part availability, and task requirements.\n",
        "\n",
        "- ðŸ“š **Retrieval-Augmented Generation (RAG)**  \n",
        "  For complex tasks like â€œWhat PC parts are best for 4K video editing?â€, the assistant uses a local vector store (ChromaDB) to ground its answers in relevant, pre-embedded documents.\n",
        "\n",
        "- ðŸ§  **Vector Search / Vector Database Integration**  \n",
        "  Embedding and indexing `.txt` documents using Gemini allows the assistant to semantically search knowledge on demand.\n",
        "\n",
        "- ðŸ§­ **Autonomous Decision Routing**  \n",
        "  The LangGraph graph architecture allows the assistant to choose between multiple next steps â€” like planning, searching, or optimizing the PC build â€” based on current context.\n",
        "\n",
        "- ðŸ”„ **Stateful, Multi-Turn Memory**  \n",
        "  Conversation state is maintained via LangGraphâ€™s `PCBuilderState`, enabling coherent, long-form dialogues across multiple turns.\n",
        "\n",
        "- ðŸ—£ï¸ **Persona / Role-Adaptive Prompting**  \n",
        "  Just like the BaristaBot example, we define a robust system prompt to shape the assistant into a helpful, focused PC-building expert that avoids distractions and guides the user carefully. Moreover, well-defined prompts and instructions are included for all of the Gemini calls that are made throughout (such as for searching)."
      ],
      "metadata": {
        "id": "DULxyoDC5fa_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ðŸ§¹ Step 1: Preparing the Environment\n",
        "\n",
        "Before we can summon our assistant, we need to clean up and prepare the environment.\n",
        "\n",
        "### What's happening here?\n",
        "\n",
        "- Remove potentially conflicting packages from the notebook environment (like `kfp`, `jupyterlab`, and others), just like the tutorial notebooks for the hackathon did.\n",
        "- Install the latest versions of libraries like `langgraph`, `langchain-google-genai`, `chromadb`, and `google-generativeai`.\n",
        "\n",
        "We follow this approach to ensure that everything works together smoothly."
      ],
      "metadata": {
        "id": "Wv-GoSOC7fLU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove conflicting packages from the base environment.\n",
        "!pip uninstall -qqy kfp jupyterlab jupyterlab-lsp libpysal thinc spacy fastai ydata-profiling google-cloud-bigquery google-generativeai bigframes google-cloud-automl google-cloud-aiplatform google-cloud-translate gcsfs pandas-gbq\n",
        "\n",
        "# Install langgraph and the packages needed.\n",
        "!pip install -qU 'langgraph==0.3.21' 'langchain-google-genai==2.1.2' 'langgraph-prebuilt==0.1.7' 'langchain-community' 'chromadb==0.6.3' 'google-genai==1.7.0'"
      ],
      "metadata": {
        "id": "JzuxdA2Kkzp5",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-21T02:46:12.257374Z",
          "iopub.execute_input": "2025-04-21T02:46:12.258252Z",
          "iopub.status.idle": "2025-04-21T02:46:24.844898Z",
          "shell.execute_reply.started": "2025-04-21T02:46:12.258225Z",
          "shell.execute_reply": "2025-04-21T02:46:24.843693Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ðŸ“¦ Step 2: Imports and Configuration\n",
        "\n",
        "Over the next few cells, we import all the necessary libraries and set up:\n",
        "\n",
        "- The Google Gemini API key\n",
        "- The main Gemini model we use (`gemini-2.0-flash`)\n",
        "- A temporary ChromaDB client, pointing to a virtual vector store\n",
        "\n",
        "Key imports included:\n",
        "- Python system and type libraries\n",
        "- LangGraph and LangChain classes for agents, tools, state graphs\n",
        "- ChromaDB for storing and retrieving embedded documents\n",
        "- Gemini SDK for using the LLM\n",
        "- Utility classes for display and debugging\n",
        "\n",
        "This imports and configurations are the \"wiring\" of our app: they will eventually connect our LLM, vector store, and LangGraph workflow graph into a working whole."
      ],
      "metadata": {
        "id": "l0u0fCZP8AxD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Imports\n",
        "import os\n",
        "import json\n",
        "import re\n",
        "import chromadb\n",
        "\n",
        "from google import genai\n",
        "from google.genai import types\n",
        "from google.api_core import retry\n",
        "\n",
        "from pprint import pprint\n",
        "from typing import List, Dict, Any, Annotated, Literal, Optional\n",
        "from collections.abc import Iterable\n",
        "from typing_extensions import TypedDict\n",
        "from IPython.display import Image, display, Markdown\n",
        "from chromadb import EmbeddingFunction, Embeddings, Documents\n",
        "\n",
        "from langchain_core.tools import tool\n",
        "from langgraph.prebuilt import ToolNode\n",
        "from langgraph.graph.message import add_messages\n",
        "from langchain_core.messages import AIMessage\n",
        "from langchain_core.documents import Document\n",
        "from langgraph.graph import StateGraph, START, END\n",
        "from langchain_core.messages.tool import ToolMessage\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from langchain.document_loaders import TextLoader\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter"
      ],
      "metadata": {
        "id": "mz6xIO6ok2ob",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-21T02:46:24.846936Z",
          "iopub.execute_input": "2025-04-21T02:46:24.847242Z",
          "iopub.status.idle": "2025-04-21T02:46:24.855524Z",
          "shell.execute_reply.started": "2025-04-21T02:46:24.847218Z",
          "shell.execute_reply": "2025-04-21T02:46:24.854626Z"
        }
      },
      "outputs": [],
      "execution_count": 2
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Set up your API key\n",
        "\n",
        "To run the following cell, you must have a Google API key.\n",
        "\n",
        "If you don't already have an API key, you can grab one from [AI Studio](https://aistudio.google.com/app/apikey). You can find [detailed instructions in the docs](https://ai.google.dev/gemini-api/docs/api-key)."
      ],
      "metadata": {
        "id": "LKaJNjuo_ovH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Setup API key for Gemini\n",
        "os.environ[\"GOOGLE_API_KEY\"] = \"your-API-key\""
      ],
      "metadata": {
        "id": "KwwS12HVk4ua",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-21T02:46:24.944917Z",
          "iopub.execute_input": "2025-04-21T02:46:24.945198Z",
          "iopub.status.idle": "2025-04-21T02:46:24.949425Z",
          "shell.execute_reply.started": "2025-04-21T02:46:24.945175Z",
          "shell.execute_reply": "2025-04-21T02:46:24.948606Z"
        }
      },
      "outputs": [],
      "execution_count": 13
    },
    {
      "cell_type": "code",
      "source": [
        "# Setup LLM model definitions\n",
        "MAIN_MODEL = 'gemini-2.0-flash'\n",
        "llm = ChatGoogleGenerativeAI(model=MAIN_MODEL)\n",
        "client = genai.Client(api_key=os.environ[\"GOOGLE_API_KEY\"])"
      ],
      "metadata": {
        "id": "4KH2vWTY3sZR",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-21T02:46:24.950452Z",
          "iopub.execute_input": "2025-04-21T02:46:24.950776Z",
          "iopub.status.idle": "2025-04-21T02:46:25.094054Z",
          "shell.execute_reply.started": "2025-04-21T02:46:24.950745Z",
          "shell.execute_reply": "2025-04-21T02:46:25.093180Z"
        }
      },
      "outputs": [],
      "execution_count": 14
    },
    {
      "cell_type": "code",
      "source": [
        "# Setup Chroma DB\n",
        "DATA_PATH = 'txt_data/'\n",
        "chroma_client = chromadb.Client()  # Temporary client\n",
        "# chroma_client = chromadb.PersistentClient(CHROMA_PATH)  # Would use this if running on local"
      ],
      "metadata": {
        "id": "gbq1x1PR66q8",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-21T02:46:25.095106Z",
          "iopub.execute_input": "2025-04-21T02:46:25.095386Z",
          "iopub.status.idle": "2025-04-21T02:46:25.103233Z",
          "shell.execute_reply.started": "2025-04-21T02:46:25.095366Z",
          "shell.execute_reply": "2025-04-21T02:46:25.102374Z"
        }
      },
      "outputs": [],
      "execution_count": 5
    },
    {
      "cell_type": "markdown",
      "source": [
        "Automated retry for Gemini to bypass the per-minute limits"
      ],
      "metadata": {
        "id": "B-0n08rn2bf5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a retry policy. The model might make multiple consecutive calls automatically\n",
        "# for a complex query, this ensures the client retries if it hits quota limits.\n",
        "from google.api_core import retry\n",
        "\n",
        "is_retriable = lambda e: (isinstance(e, genai.errors.APIError) and e.code in {429, 503})\n",
        "\n",
        "if not hasattr(genai.models.Models.generate_content, '__wrapped__'):\n",
        "  genai.models.Models.generate_content = retry.Retry(\n",
        "      predicate=is_retriable)(genai.models.Models.generate_content)"
      ],
      "metadata": {
        "id": "GsxyTXJY2gcg",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-21T02:46:25.104328Z",
          "iopub.execute_input": "2025-04-21T02:46:25.104659Z",
          "iopub.status.idle": "2025-04-21T02:46:25.125380Z",
          "shell.execute_reply.started": "2025-04-21T02:46:25.104618Z",
          "shell.execute_reply": "2025-04-21T02:46:25.124533Z"
        }
      },
      "outputs": [],
      "execution_count": 15
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ðŸ§  Step 3: Defining the Assistant's Context and Personality\n",
        "\n",
        "This section defines some key variables:\n",
        "\n",
        "- `PCBuilderState`: The core of LangGraph: its state. This keeps track of the conversation history, the PC requirements, the user's budget, and the assistant's build recommendations.\n",
        "- `PC_BUILDER_SYSINT`: An in-depth, powerful **system instruction** prompt that turns the assistant into a disciplined, helpful PC building expert.\n",
        "\n",
        "### Why does this matter?\n",
        "\n",
        "This is where the assistant **gets its personality and rules.** It's engineered to:\n",
        "\n",
        "- Ask about budget and use cases\n",
        "- Understand whether you want a prebuilt or custom PC\n",
        "- Search online for relevant parts\n",
        "- Rank those parts based on price and performance\n",
        "- Return recommendations that make sense for your budget\n",
        "\n",
        "All without ever needing you to know what a PCIe lane is. ðŸ™Œ\n",
        "\n",
        "This is how the agent knows how to guide users, when to search for parts, how to explain results, and what actions it should or shouldn't take.\n",
        "\n",
        "---\n",
        "\n",
        "### ðŸ‘‹ Step 3.1: Welcoming the User\n",
        "\n",
        "Just like in day three's BaristaBot example, we also make sure to start the chat with a clear and friendly welcome message:\n",
        "\n",
        "> â€œWelcome to the PC Builder Assistant! (Type `q` to quit)...â€\n",
        "\n",
        "This message invites the user into the experience and gets them started with sharing their goals and budget."
      ],
      "metadata": {
        "id": "RrAM62478aSF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class PCBuilderState(TypedDict):\n",
        "    \"\"\"State representing the PC builder conversation.\"\"\"\n",
        "\n",
        "    # The chat conversation. This preserves the conversation history\n",
        "    # between nodes. The `add_messages` annotation indicates to LangGraph\n",
        "    # that state is updated by appending returned messages, not replacing\n",
        "    # them.\n",
        "    messages: Annotated[list, add_messages]\n",
        "\n",
        "    # User requirements\n",
        "    # requirements: Dict[str, Any]\n",
        "    requirements: list[str]\n",
        "\n",
        "    # Budget information\n",
        "    budget: str\n",
        "\n",
        "    # Current recommendations for parts\n",
        "    recommendations: list[Dict[str, Any]]\n",
        "\n",
        "    # Flag for whether the build is complete\n",
        "    build_complete: bool\n",
        "\n",
        "\n",
        "# The system instruction defines how the chatbot is expected to behave and includes\n",
        "# rules for when to call different functions, as well as rules for the conversation, such\n",
        "# as tone and what is permitted for discussion.\n",
        "PC_BUILDER_SYSINT = (\n",
        "    \"system\",  # 'system' indicates the message is a system instruction.\n",
        "    \"\"\"\n",
        "You are a PC Builder Assistant, an expert in computer hardware and building custom PCs.\n",
        "Your goal is to help users find the perfect PC build based on their budget and requirements.\n",
        "\n",
        "You should:\n",
        "1. Ask about their budget and use case (gaming, office work, content creation, etc.)\n",
        "2. Determine if they need a custom build or pre-built system.\n",
        "3. For pre-built systems, ask if desktops or laptops or either are preferred.\n",
        "4. For custom builds, if they have existing hardware, ask what parts they want to upgrade.\n",
        "\n",
        "Once you have a general idea of what the user is looking for, you must formulate that\n",
        "into a structured list of concise individual requirements and call the update_plans tool,\n",
        "providing the budget and list of requirements. For custom build devices, if the user\n",
        "mentions wanting the PC for a specific task that is hardware intensive, such as playing a\n",
        "video game or video editing or training AI models or such, then first use the\n",
        "search_task_requirements tool to find the hardware requirements for the specified task,\n",
        "and only then update the user requirements using the update_plans tool, with the extra\n",
        "hardware requirements added to it.\n",
        "\n",
        "For pre-builds, even if there are potentially-intensive task requirements, you can call\n",
        "the update_plans tool directly, adding the task requirements as part of the main requirements.\n",
        "\n",
        "Once the requirements are all logged by the tool, summarize the requirements back to the\n",
        "user in a couple of sentences using the latest requirements list and budget returned by\n",
        "your last call of the tool.\n",
        "\n",
        "If the user wants to change something in their plans, send a new list of requirements back\n",
        "to update_plans. If they want to start from scratch, use the clear_plan tool to remove all\n",
        "budget and requirements, then walk through the requirements gathering steps again.\n",
        "\n",
        "If the user confirms their requirements are final and they want a pre-built device, use\n",
        "search_prebuilt tool to find pre-built devices fitting the user criteria, which will be\n",
        "given to you in a structured JSON format. If the JSON has formatting errors and you cannot\n",
        "understand it, recall the search_prebuilt tool. If the JSON can be understood, call the\n",
        "rank_prebuilds tool and provide it the exact same JSON (it will not work without JSON!) to\n",
        "get a final list of recommendations and then render this neatly in markdown for the user\n",
        "to browse.\n",
        "\n",
        "If the user confirms their requirements are final and they want a custom build, use the\n",
        "lookup_parts_needed tool to get a list of the parts required. Once you have the parts,\n",
        "call the search_custom_parts tool to get a list of parts, which should be in a structured\n",
        "JSON format. If the JSON has formatting errors and you cannot understand it, recall the\n",
        "search_custom_parts tool. If the JSON can be understood, call the rank_parts tool and\n",
        "provide it the exact same JSON (it will not work without JSON!) to get a final list of\n",
        "parts for the user. Render this neatly in markdown for the user to browse.\n",
        "\n",
        "The user may have additional questions about the parts or building process, which you must\n",
        "expand upon if asked. However, if they express satisfaction or thank you and seem ready to\n",
        "leave after you've made your recommendations, then remind them they can quit by typing `q`\n",
        "or `quit`.\n",
        "\n",
        "Stay focused on PC building. If users ask about unrelated topics, gently redirect them.\n",
        "\"\"\",\n",
        ")\n",
        "\n",
        "# This is the message with which the system opens the conversation.\n",
        "WELCOME_MSG = \"Welcome to the PC Builder Assistant! (Type `q` to quit). I'll help you find the perfect computer based on your needs and budget. Could you tell me your budget and what you'll be using this PC for? (Gaming, office work, content creation, etc.)\""
      ],
      "metadata": {
        "id": "86ysZFyXk6W6",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-21T02:46:25.126444Z",
          "iopub.execute_input": "2025-04-21T02:46:25.127069Z",
          "iopub.status.idle": "2025-04-21T02:46:25.146057Z",
          "shell.execute_reply.started": "2025-04-21T02:46:25.127038Z",
          "shell.execute_reply": "2025-04-21T02:46:25.145175Z"
        }
      },
      "outputs": [],
      "execution_count": 7
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ðŸ“š Step 4: ChromaDB â€” Creating a Local Knowledge Base\n",
        "\n",
        "This section builds local memory for the assistant using `.txt` documents that contain information about PC parts and hardware specs.\n",
        "\n",
        "### Here's what happens:\n",
        "\n",
        "- We define a custom `GeminiEmbeddingFunction` that can embed both queries and documents\n",
        "- We clean and chunk `.txt` files into small passages\n",
        "- We embed and store these passages in ChromaDB using Gemini to create vector embeddings\n",
        "\n",
        "This lets the assistant \"remember\" what kind of specs matter for potentially-intensive tasks (such as gaming, content creation, training AI, etc.) and answer questions like:\n",
        "\n",
        "> \"What parts do I need to play Cyberpunk on Ultra at 1440p?\"\n",
        "\n",
        "This vector store will work by retrieving relevant info about tasks (e.g., \"video gaming\", \"high fidelity\") to build accurate hardware parts lists."
      ],
      "metadata": {
        "id": "wzmRUqp-9wAT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "_Note_ - The `GeminiEmbeddingFunction` is taken from **Day 2 - Document Q&A with RAG using Chroma**...no need to reinvent the wheel! ðŸ˜„"
      ],
      "metadata": {
        "id": "mywQYpBV5Dcv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class GeminiEmbeddingFunction(EmbeddingFunction):\n",
        "    # Specify whether to generate embeddings for documents, or queries\n",
        "    document_mode = True\n",
        "\n",
        "    @retry.Retry(predicate=is_retriable)\n",
        "    def __call__(self, input_: Documents) -> Embeddings:\n",
        "        if self.document_mode:\n",
        "            embedding_task = 'retrieval_document'\n",
        "        else:\n",
        "            embedding_task = 'retrieval_query'\n",
        "\n",
        "        response = client.models.embed_content(\n",
        "            model='models/text-embedding-004',\n",
        "            contents=input_,\n",
        "            config=types.EmbedContentConfig(\n",
        "                task_type=embedding_task\n",
        "            )\n",
        "        )\n",
        "        return [em.values for em in response.embeddings]"
      ],
      "metadata": {
        "id": "9RWOEGYB7NSF",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-21T02:46:25.147072Z",
          "iopub.execute_input": "2025-04-21T02:46:25.147866Z",
          "iopub.status.idle": "2025-04-21T02:46:25.170846Z",
          "shell.execute_reply.started": "2025-04-21T02:46:25.147844Z",
          "shell.execute_reply": "2025-04-21T02:46:25.169871Z"
        }
      },
      "outputs": [],
      "execution_count": 16
    },
    {
      "cell_type": "code",
      "source": [
        "# Make sure to remove any noisy text characters and strings from what goes into the embeddings.\n",
        "def clean_extracted_text(text: str) -> str:\n",
        "    for char in ['~', 'Â©', '_', ';:;', 'Â®', '#', '@', ' ', 'Â ']:\n",
        "        text = text.replace(char, '')\n",
        "    return text"
      ],
      "metadata": {
        "id": "HJYUZKDG8ic2",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-21T02:46:25.182309Z",
          "iopub.execute_input": "2025-04-21T02:46:25.182999Z",
          "iopub.status.idle": "2025-04-21T02:46:25.188544Z",
          "shell.execute_reply.started": "2025-04-21T02:46:25.182960Z",
          "shell.execute_reply": "2025-04-21T02:46:25.187635Z"
        }
      },
      "outputs": [],
      "execution_count": 17
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Text Chunking for RAG Systems\n",
        "\n",
        "This code demonstrates document processing for (RAG) systems. It loads a text file and splits it into manageable chunks using LangChain's `RecursiveCharacterTextSplitter`.\n",
        "\n",
        "The `RecursiveCharacterTextSplitter` divides text into smaller segments (500 characters each with 60 character overlap) which is crucial for RAG applications because:\n",
        "\n",
        "\n",
        "1.   Chunking allows for more precise retrieval of relevant information\n",
        "2.   Properly sized chunks improve embedding quality and semantic search accuracy\n",
        "\n",
        "3.   Overlapping chunks preserve context across segment boundaries\n",
        "4.   Smaller chunks enable more efficient vector similarity searches\n",
        "\n",
        "This chunking process prepares text data for embedding and vector storage, forming the foundation of an effective RAG system that can retrieve contextually relevant information when responding to queries.\n"
      ],
      "metadata": {
        "id": "FfF73wAjmMYe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_chunks(file_path: str) -> list[Document]:\n",
        "    loader = TextLoader(file_path)\n",
        "    raw_documents = loader.load()\n",
        "    print('Document loaded:', len(raw_documents))\n",
        "\n",
        "    # splitting the document\n",
        "    text_splitter = RecursiveCharacterTextSplitter(\n",
        "        chunk_size=500,\n",
        "        chunk_overlap=60,\n",
        "        length_function=len,\n",
        "        is_separator_regex=False,\n",
        "    )\n",
        "\n",
        "    chunks = text_splitter.split_documents(raw_documents)\n",
        "    print('Number of chunks after splitting:', len(chunks))\n",
        "    return chunks"
      ],
      "metadata": {
        "id": "sTA7PQTB9_OW",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-21T02:46:25.189540Z",
          "iopub.execute_input": "2025-04-21T02:46:25.189864Z",
          "iopub.status.idle": "2025-04-21T02:46:25.210270Z",
          "shell.execute_reply.started": "2025-04-21T02:46:25.189840Z",
          "shell.execute_reply": "2025-04-21T02:46:25.209278Z"
        }
      },
      "outputs": [],
      "execution_count": 18
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Batch Embedding for Vector Database Storage\n",
        "\n",
        "This code implements a batch processing approach for embedding document chunks into a ChromaDB vector database. Key aspects include:\n",
        "\n",
        "1. **Batch Processing**: The function processes chunks in smaller batches (default size: 50) rather than all at once. This strategy is specifically designed to avoid hitting API rate limits on the free tier of Google's genai API.\n",
        "\n",
        "2. **Text Preparation**: Each chunk's content is cleaned via the `clean_extracted_text` function before embedding.\n",
        "\n",
        "3. **Metadata Management**: Each document chunk maintains its metadata and receives a unique ID based on the source file name and chunk position.\n",
        "\n",
        "4. **Resource Efficiency**: Batch processing not only prevents API limit issues but also optimizes memory usage and provides better progress tracking through status updates.\n",
        "\n",
        "This approach demonstrates a practical solution for working with external embedding APIs that have usage restrictions while efficiently populating a vector database for RAG applications."
      ],
      "metadata": {
        "id": "Fd8wPwuwpwBc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def batch_embed_chunks(file_, file_path: str, chunks_: list[Document], batch_size: int=50) -> None:\n",
        "    # Process chunks in smaller batches to manage resources and API limits\n",
        "    for i in range(0, len(chunks_), batch_size):\n",
        "        batch_chunks = chunks_[i:i + batch_size]\n",
        "        batch_documents = [clean_extracted_text(chunk.page_content) for chunk in batch_chunks]\n",
        "\n",
        "        try:\n",
        "            batch_embeddings = embed_fn(batch_documents)\n",
        "        except Exception as exc:\n",
        "            print(f'Error generating embeddings for batch {i // batch_size} from {file_path}: {exc}')\n",
        "            continue\n",
        "\n",
        "        batch_metadata = [chunk.metadata for chunk in batch_chunks]\n",
        "        batch_ids = [\n",
        "            f'{os.path.splitext(file_)[0]}_chunk_{i + j}'\n",
        "            for j in range(len(batch_chunks))\n",
        "        ]\n",
        "\n",
        "        db.add(\n",
        "            documents=batch_documents,\n",
        "            metadatas=batch_metadata,\n",
        "            ids=batch_ids\n",
        "        )\n",
        "        print(f'Added batch {i // batch_size + 1} from {file_path} to ChromaDB. Total: {db.count()}.')"
      ],
      "metadata": {
        "id": "9Pp90Ziw-iQd",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-21T02:46:25.211177Z",
          "iopub.execute_input": "2025-04-21T02:46:25.211429Z",
          "iopub.status.idle": "2025-04-21T02:46:25.227863Z",
          "shell.execute_reply.started": "2025-04-21T02:46:25.211410Z",
          "shell.execute_reply": "2025-04-21T02:46:25.226963Z"
        }
      },
      "outputs": [],
      "execution_count": 19
    },
    {
      "cell_type": "code",
      "source": [
        "# Embed the chunks of text via batching in Gemini and add them to the Chroma vector store\n",
        "embed_fn = GeminiEmbeddingFunction()\n",
        "embed_fn.document_mode = True\n",
        "db = chroma_client.get_or_create_collection(name='building_pcs', embedding_function=embed_fn)\n",
        "\n",
        "for file in os.listdir(DATA_PATH):\n",
        "    if not file.endswith('.txt'):\n",
        "        continue\n",
        "\n",
        "    print('File being worked on:', file)\n",
        "    path = os.path.join(DATA_PATH, file)\n",
        "\n",
        "    chunks = get_chunks(path)\n",
        "    batch_embed_chunks(file, path, chunks)\n",
        "\n",
        "embed_fn.document_mode = False"
      ],
      "metadata": {
        "id": "t8GkILTK-kS1",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-21T02:46:25.228944Z",
          "iopub.execute_input": "2025-04-21T02:46:25.229330Z",
          "iopub.status.idle": "2025-04-21T02:46:47.294717Z",
          "shell.execute_reply.started": "2025-04-21T02:46:25.229308Z",
          "shell.execute_reply": "2025-04-21T02:46:47.293745Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Utility function to query the vector store used later\n",
        "def get_query_result(query: str) -> list[str]:\n",
        "    fn = GeminiEmbeddingFunction()\n",
        "    fn.document_mode = False\n",
        "    r = chroma_client.get_collection(\n",
        "        name='building_pcs',\n",
        "        embedding_function=fn).query(query_texts=[query], n_results=5)\n",
        "    [ans] = r['documents']\n",
        "    return ans"
      ],
      "metadata": {
        "id": "UDPQ149Y-mau",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-21T02:46:47.295863Z",
          "iopub.execute_input": "2025-04-21T02:46:47.296624Z",
          "iopub.status.idle": "2025-04-21T02:46:47.302237Z",
          "shell.execute_reply.started": "2025-04-21T02:46:47.296595Z",
          "shell.execute_reply": "2025-04-21T02:46:47.301359Z"
        }
      },
      "outputs": [],
      "execution_count": 21
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ðŸ§® Step 5: Utility Functions for Reasoning and Cleanup\n",
        "\n",
        "These functions include:\n",
        "\n",
        "- `parse_price()` to convert price strings to numbers\n",
        "- `strip_json_wrapper()` to handle Gemini's structured, JSON-formatted responses cleanly\n",
        "- `return_clean_json()` to run LLM calls which return structured JSON and re-run them automatically if the JSON output is malformed\n",
        "- `score_specs()` and `extract_specs_score()` to evaluate how good a device or part is via a custom-written, performance-to-cost ratio scoring algorithm based on our evaluation and opinions of how PC parts stack up to each other\n",
        "- `rank_devices()` and `rank_options_available()` to return top 3 best-value choices\n",
        "\n",
        "This logic powers the **reasoning logic** behind our assistant's choice of parts â€” converting unstructured search results into smart, data-backed recommendations.\n"
      ],
      "metadata": {
        "id": "hJ9Fb0YZ-PtF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Utility function to help with parsing prices from strings.\n",
        "def parse_price(price_str: str) -> float:\n",
        "    \"\"\"Extract numeric price value from the provided string.\"\"\"\n",
        "    if price_str == 'N/A':\n",
        "        return 0.0\n",
        "\n",
        "    # Extract digits and decimal value\n",
        "    price_match = re.search(r'[\\d,.]+', price_str)\n",
        "    if not price_match:\n",
        "        return 0.0\n",
        "\n",
        "    # Remove commas and convert to float\n",
        "    price_digits = price_match.group(0).replace(',', '')\n",
        "    if price_digits and not price_digits.isspace():\n",
        "        return float(price_digits)"
      ],
      "metadata": {
        "id": "G_TpLz4OdOGc",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-21T02:46:47.303391Z",
          "iopub.execute_input": "2025-04-21T02:46:47.303665Z",
          "iopub.status.idle": "2025-04-21T02:46:47.323180Z",
          "shell.execute_reply.started": "2025-04-21T02:46:47.303645Z",
          "shell.execute_reply": "2025-04-21T02:46:47.322171Z"
        }
      },
      "outputs": [],
      "execution_count": 22
    },
    {
      "cell_type": "code",
      "source": [
        "# Removes the start and end of JSON codeblock formatting that the LLMs occasionally wrap JSON strings with.\n",
        "def strip_json_wrapper(json_str: str) -> str:\n",
        "    return json_str.replace('```json\\n', '').replace('\\n```', '')"
      ],
      "metadata": {
        "id": "ohOYd1lSlzqi",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-21T02:46:47.323992Z",
          "iopub.execute_input": "2025-04-21T02:46:47.324376Z",
          "iopub.status.idle": "2025-04-21T02:46:47.340821Z",
          "shell.execute_reply.started": "2025-04-21T02:46:47.324348Z",
          "shell.execute_reply": "2025-04-21T02:46:47.339893Z"
        }
      },
      "outputs": [],
      "execution_count": 23
    },
    {
      "cell_type": "code",
      "source": [
        "# Check if the model returns a malformed JSON string and re-run it to curb downstream exceptions.\n",
        "def return_clean_json(original_function, *args, **kwargs):\n",
        "    counter = 1\n",
        "    while True:\n",
        "        try:\n",
        "            parsed_json = json.loads(strip_json_wrapper(original_function(*args, **kwargs)))\n",
        "            return json.dumps(parsed_json, indent=4, ensure_ascii=False)\n",
        "        except json.JSONDecodeError:\n",
        "            if counter > 5:\n",
        "                print('The model keeps returning an invalid JSON string. Stopping program for now...')\n",
        "                return\n",
        "            print(f'Invalid JSON detected. Re-running method ({counter})...')\n",
        "            counter += 1\n",
        "        except Exception as e:\n",
        "            # This function only handles JSON errors. Anything else is outside its purview.\n",
        "            return json.dumps({'error': str(e)})"
      ],
      "metadata": {
        "id": "U_V7uXoAchVB",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-21T02:46:47.341781Z",
          "iopub.execute_input": "2025-04-21T02:46:47.342109Z",
          "iopub.status.idle": "2025-04-21T02:46:47.358990Z",
          "shell.execute_reply.started": "2025-04-21T02:46:47.342087Z",
          "shell.execute_reply": "2025-04-21T02:46:47.358005Z"
        }
      },
      "outputs": [],
      "execution_count": 24
    },
    {
      "cell_type": "code",
      "source": [
        "# Helper functions required to rank pre-built devices\n",
        "def score_specs(specs: str) -> int:\n",
        "    \"\"\"\n",
        "    Analyze specifications to score device performance.\n",
        "    Higher scores indicate better performance.\n",
        "    \"\"\"\n",
        "    score = 0\n",
        "    specs = specs.lower()\n",
        "\n",
        "    # CPU scoring using their tier; we use the tier of the processor as higher\n",
        "    # tiers typically translates to higher clock speeds, more cache, etc.\n",
        "    if 'ryzen 7' in specs or 'i7' in specs:\n",
        "        score += 80\n",
        "    elif 'ryzen 5' in specs or 'i5' in specs:\n",
        "        score += 60\n",
        "    elif 'ryzen 3' in specs or 'i3' in specs:\n",
        "        score += 40\n",
        "\n",
        "    # Generation bonus; this is the sub category of the tier which, again,\n",
        "    # typically translates to higher clock speeds, more cache, etc.\n",
        "    if '5700' in specs:\n",
        "        score += 20\n",
        "    elif '5600' in specs or '5500' in specs:\n",
        "        score += 15\n",
        "    elif '4500' in specs or '4600' in specs:\n",
        "        score += 10\n",
        "\n",
        "    # RAM scoring; we use the number of GBs because the more a system has access\n",
        "    # to the more simultaneous operations it can support\n",
        "    ram_match = re.search(r'(\\d+)gb', specs.replace(' ', ''))\n",
        "    if ram_match:\n",
        "        ram_size = int(ram_match.group(1))\n",
        "        if ram_size >= 32:\n",
        "            score += 50\n",
        "        elif ram_size >= 16:\n",
        "            score += 30\n",
        "        elif ram_size >= 8:\n",
        "            score += 15\n",
        "\n",
        "    # Storage scoring using their capacity and the type of storage; nvme vs\n",
        "    # traditional SATA SSD\n",
        "    if '1tb' in specs.replace(' ', ''):\n",
        "        score += 30\n",
        "    elif '500gb' in specs.replace(' ', '') or '512gb' in specs.replace(' ', ''):\n",
        "        score += 20\n",
        "\n",
        "    if 'nvme' in specs or 'ssd' in specs:\n",
        "        score += 20\n",
        "\n",
        "    # GPU scoring with nvidia series above AMD; while this will not always be\n",
        "    # true, the function would be too complicated to account for all GPUs so,\n",
        "    # decisions had to be made...almost any designated GPU will be better than\n",
        "    # onboard however\n",
        "    if 'rtx 3080' in specs:\n",
        "        score += 100\n",
        "    elif 'rtx 3070' in specs:\n",
        "        score += 80\n",
        "    elif 'rtx 3060' in specs:\n",
        "        score += 70\n",
        "    elif 'gtx 1650' in specs:\n",
        "        score += 40\n",
        "    elif 'radeon' in specs or 'onboard' in specs:\n",
        "        score += 20\n",
        "\n",
        "    return score\n",
        "\n",
        "\n",
        "def get_value_ratio(device: Dict[str, Any], budget: float) -> float:\n",
        "    \"\"\"\n",
        "    Calculate value ratio based on specs score and price.\n",
        "    Returns 0 if device exceeds budget.\n",
        "    \"\"\"\n",
        "    price = parse_price(device['price'])\n",
        "    if price == None or price > budget:\n",
        "        return 0\n",
        "\n",
        "    specs_score = score_specs(device['specifications'])\n",
        "\n",
        "    # Calculate value ratio (specs score per unit of price)\n",
        "    # Higher ratio means better value\n",
        "    if price > 0:\n",
        "        return specs_score / price\n",
        "    return 0\n",
        "\n",
        "\n",
        "def rank_devices(devices_json: str, budget: float, return_json: Optional[bool] = False) -> Any:\n",
        "    \"\"\"\n",
        "    Rank devices based on specifications and price within budget.\n",
        "    Returns JSON string with top 3 devices.\n",
        "    \"\"\"\n",
        "    devices = json.loads(devices_json)\n",
        "\n",
        "    # Sometimes the model returns JSON as an object/map, with a single field mapping to the list,\n",
        "    # instead of a list as the parent element. This attempts to handle the two fieldnames it usually uses.\n",
        "    if isinstance(devices, dict):\n",
        "        fieldname = list(devices.keys())[0]\n",
        "        devices = devices[fieldname]\n",
        "\n",
        "    for device in devices:\n",
        "        device['value_ratio'] = get_value_ratio(device, budget)\n",
        "\n",
        "    # Sort devices by assigned value ratios descending\n",
        "    ranked_devices = sorted(devices, key=lambda x: x['value_ratio'], reverse=True)\n",
        "\n",
        "    # Select top 3 within budget\n",
        "    top_devices = [\n",
        "        {k: v for k, v in device.items() if k != 'value_ratio'}\n",
        "        for device in ranked_devices if parse_price(device['price']) is not None and (parse_price(device['price']) <= budget)\n",
        "    ]\n",
        "\n",
        "    if return_json:\n",
        "        return json.dumps(top_devices[:3], indent=4, ensure_ascii=False)\n",
        "    return top_devices"
      ],
      "metadata": {
        "id": "tCsnnXgSCcw6",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-21T02:46:47.359948Z",
          "iopub.execute_input": "2025-04-21T02:46:47.360176Z",
          "iopub.status.idle": "2025-04-21T02:46:47.383593Z",
          "shell.execute_reply.started": "2025-04-21T02:46:47.360159Z",
          "shell.execute_reply": "2025-04-21T02:46:47.382724Z"
        }
      },
      "outputs": [],
      "execution_count": 25
    },
    {
      "cell_type": "code",
      "source": [
        "# Parts scorer function\n",
        "def extract_specs_score(specs: str, part_type: str) -> float:\n",
        "    \"\"\"\n",
        "    Extract a numerical score from specifications based on part type.\n",
        "    Higher score means better performance.\n",
        "    \"\"\"\n",
        "    score = 0\n",
        "    specs_lower = specs.lower()\n",
        "\n",
        "    # CPU scoring\n",
        "    if any(cpu_term in part_type.lower() for cpu_term in [\"cpu\", \"processor\", \"core\"]):\n",
        "        # Score based on cores; more cores means more multitasking\n",
        "        core_match = re.search(r'(\\d+)\\s*cores?', specs, re.IGNORECASE) or re.search(r'(\\d+)[\\s-]*core', specs, re.IGNORECASE)\n",
        "        if core_match:\n",
        "            score += int(core_match.group(1)) * 10\n",
        "\n",
        "        # Score based on threads; more threads means more instructions that can\n",
        "        # be processed simultaneously\n",
        "        thread_match = re.search(r'(\\d+)\\s*threads?', specs, re.IGNORECASE)\n",
        "        if thread_match:\n",
        "            score += int(thread_match.group(1)) * 5\n",
        "\n",
        "        # Score based on clock speed; more clock speed means instructions are\n",
        "        # processed faster\n",
        "        clock_match = re.search(r'(\\d+\\.\\d+)\\s*GHz', specs, re.IGNORECASE)\n",
        "        if clock_match:\n",
        "            score += float(clock_match.group(1)) * 20\n",
        "\n",
        "        # Score based on cache; more cache means more repetative tasks don't\n",
        "        # need the main memory to be accessed as information is stored in the cache\n",
        "        cache_match = re.search(r'(\\d+)\\s*MB\\s*[Cc]ache', specs)\n",
        "        if cache_match:\n",
        "            score += int(cache_match.group(1)) * 2\n",
        "\n",
        "        # Bonus for newer generations\n",
        "        if '14' in specs:\n",
        "            score += 50\n",
        "        elif '13' in specs:\n",
        "            score += 40\n",
        "        elif '12' in specs:\n",
        "            score += 30\n",
        "        elif '11' in specs:\n",
        "            score += 20\n",
        "        elif '10' in specs:\n",
        "            score += 10\n",
        "\n",
        "    # GPU scoring\n",
        "    elif any(gpu_term in part_type.lower() for gpu_term in [\"gpu\", \"graphics\", \"video card\", \"geforce\", \"radeon\"]):\n",
        "        # Score based on CUDA cores or Stream Processors; more means more parallel execution\n",
        "        cuda_match = re.search(r'(\\d+,?\\d*)\\s*CUDA', specs, re.IGNORECASE)\n",
        "        if cuda_match:\n",
        "            cuda_cores = int(cuda_match.group(1).replace(',', ''))\n",
        "            score += cuda_cores / 100\n",
        "\n",
        "        stream_match = re.search(r'(\\d+,?\\d*)\\s*Stream\\s*Processors', specs, re.IGNORECASE)\n",
        "        if stream_match:\n",
        "            stream_processors = int(stream_match.group(1).replace(',', ''))\n",
        "            score += stream_processors / 80\n",
        "\n",
        "        # Score based on memory; more data can be held for processing\n",
        "        memory_match = re.search(r'(\\d+)\\s*GB', specs, re.IGNORECASE)\n",
        "        if memory_match:\n",
        "            score += int(memory_match.group(1)) * 10\n",
        "\n",
        "        # Score based on memory speed; the faster it can access required data\n",
        "        speed_match = re.search(r'(\\d+\\.\\d+)\\s*Gbps', specs, re.IGNORECASE)\n",
        "        if speed_match:\n",
        "            score += float(speed_match.group(1)) * 2\n",
        "\n",
        "        # Score based on memory bus width; faster data transfer which means less\n",
        "        # likely for intensive tasks like gaming to get bottlenecked\n",
        "        bus_match = re.search(r'(\\d+)[\\s-]*bit', specs, re.IGNORECASE)\n",
        "        if bus_match:\n",
        "            score += int(bus_match.group(1)) / 10\n",
        "\n",
        "        # Score based on clock speeds; faster it can process instructions\n",
        "        clock_match = re.search(r'(\\d+)\\s*MHz', specs, re.IGNORECASE)\n",
        "        if clock_match:\n",
        "            score += int(clock_match.group(1)) / 100\n",
        "\n",
        "    # RAM scoring\n",
        "    elif any(ram_term in part_type.lower() for ram_term in [\"ram\", \"memory\", \"ddr\"]):\n",
        "        # Score based on capacity; more capacity means more the more\n",
        "        # simultaneous tasks that can run\n",
        "        capacity_match = re.search(r'(\\d+)\\s*GB', specs, re.IGNORECASE)\n",
        "        if capacity_match:\n",
        "            score += int(capacity_match.group(1)) * 10\n",
        "\n",
        "        # Score based on speed; higher speeds means faster access to and from the CPU\n",
        "        speed_match = re.search(r'(\\d+)\\s*MHz', specs, re.IGNORECASE) or re.search(r'DDR\\d+-(\\d+)', specs, re.IGNORECASE)\n",
        "        if speed_match:\n",
        "            score += int(speed_match.group(1)) / 100\n",
        "\n",
        "        # Score based on CAS latency; determines how quickly RAM can respond to\n",
        "        # a data request from CPU\n",
        "        cas_match = re.search(r'CL(\\d+)', specs, re.IGNORECASE)\n",
        "        if cas_match:\n",
        "            # Lower CAS is better, so we invert the relationship\n",
        "            score += 20 - int(cas_match.group(1))\n",
        "\n",
        "        # Type of RAM bonus; generational improvements\n",
        "        if 'ddr5' in specs_lower:\n",
        "            score += 50\n",
        "        elif 'ddr4' in specs_lower:\n",
        "            score += 30\n",
        "        elif 'ddr3' in specs_lower:\n",
        "            score += 10\n",
        "\n",
        "    # Storage scoring\n",
        "    elif any(storage_term in part_type.lower() for storage_term in [\"ssd\", \"hdd\", \"storage\", \"drive\", \"nvme\"]):\n",
        "        # Score based on capacity\n",
        "        tb_match = re.search(r'(\\d+)\\s*TB', specs, re.IGNORECASE)\n",
        "        if tb_match:\n",
        "            score += int(tb_match.group(1)) * 100\n",
        "\n",
        "        gb_match = re.search(r'(\\d+)\\s*GB', specs, re.IGNORECASE)\n",
        "        if gb_match:\n",
        "            score += int(gb_match.group(1)) / 10\n",
        "\n",
        "        # Score based on read/write speeds; translates to faster access\n",
        "        read_match = re.search(r'(\\d+,?\\d*)\\s*MB\\/s read', specs, re.IGNORECASE) or re.search(r'read:?\\s*(\\d+,?\\d*)\\s*MB\\/s', specs, re.IGNORECASE)\n",
        "        if read_match:\n",
        "            score += int(read_match.group(1).replace(',', '')) / 100\n",
        "\n",
        "        # Type bonus; the type of storage can heavily influence read/write speeds\n",
        "        # eg an M.2 which is directly installed in the motherboard will be\n",
        "        # faster than a SATA SSD\n",
        "        if any(fast_storage in specs_lower for fast_storage in ['nvme', 'm.2', 'pcie 4.0', 'pcie4']):\n",
        "            score += 50\n",
        "        elif 'ssd' in specs_lower:\n",
        "            score += 30\n",
        "        elif 'hdd' in specs_lower:\n",
        "            score += 10\n",
        "\n",
        "    # Motherboard scoring\n",
        "    elif any(mb_term in part_type.lower() for mb_term in [\"motherboard\", \"mainboard\", \"mobo\"]):\n",
        "        # Score based on chipset and board type\n",
        "        chipset_score = 0\n",
        "        if any(x in specs_lower for x in ['z790', 'x670', 'x670e']):\n",
        "            chipset_score = 50\n",
        "        elif any(x in specs_lower for x in ['z690', 'x570', 'b650']):\n",
        "            chipset_score = 40\n",
        "        elif any(x in specs_lower for x in ['b550', 'z590', 'b560']):\n",
        "            chipset_score = 30\n",
        "        score += chipset_score\n",
        "\n",
        "        # Score based on memory support; better memory that can be supported\n",
        "        # impacts CPU performance and overall PC performance\n",
        "        if 'ddr5' in specs_lower:\n",
        "            score += 30\n",
        "        elif 'ddr4' in specs_lower:\n",
        "            score += 15\n",
        "\n",
        "        # Score based on PCIe support; higher pcie support means faster data\n",
        "        # transfer and more efficient power transfer between components like the GPU\n",
        "        if 'pcie 5.0' in specs_lower or 'pcie5' in specs_lower:\n",
        "            score += 30\n",
        "        elif 'pcie 4.0' in specs_lower or 'pcie4' in specs_lower:\n",
        "            score += 20\n",
        "        elif 'pcie 3.0' in specs_lower or 'pcie3' in specs_lower:\n",
        "            score += 10\n",
        "\n",
        "        # Score based on connectivity built-in; removes the requirement of\n",
        "        # installing a NIC something a novice is likely to miss\n",
        "        if 'wifi' in specs_lower:\n",
        "            score += 15\n",
        "        if 'bluetooth' in specs_lower:\n",
        "            score += 10\n",
        "        if 'usb 3' in specs_lower or 'usb-c' in specs_lower:\n",
        "            score += 15\n",
        "\n",
        "    # PSU scoring\n",
        "    elif any(psu_term in part_type.lower() for psu_term in [\"psu\", \"power supply\", \"power\"]):\n",
        "        # Score based on wattage\n",
        "        wattage_match = re.search(r'(\\d+)\\s*W', specs, re.IGNORECASE) or re.search(r'(\\d+)\\s*watt', specs, re.IGNORECASE)\n",
        "        if wattage_match:\n",
        "            score += int(wattage_match.group(1)) / 10\n",
        "\n",
        "        # Score based on power transfer efficiency\n",
        "        if '80+ titanium' in specs_lower:\n",
        "            score += 50\n",
        "        elif '80+ platinum' in specs_lower:\n",
        "            score += 40\n",
        "        elif '80+ gold' in specs_lower:\n",
        "            score += 30\n",
        "        elif '80+ silver' in specs_lower:\n",
        "            score += 20\n",
        "        elif '80+ bronze' in specs_lower:\n",
        "            score += 10\n",
        "\n",
        "        # Score based on modularity; full modular/semi-modular makes cable management easier\n",
        "        if 'full modular' in specs_lower or 'fully modular' in specs_lower:\n",
        "            score += 20\n",
        "        elif 'semi modular' in specs_lower or 'semi-modular' in specs_lower:\n",
        "            score += 10\n",
        "\n",
        "    # Case scoring\n",
        "    elif any(case_term in part_type.lower() for case_term in [\"case\", \"chassis\", \"tower\"]):\n",
        "        # Score based on form factor; larger the case the more you can fit inside!\n",
        "        if 'full tower' in specs_lower:\n",
        "            score += 30\n",
        "        elif 'mid tower' in specs_lower:\n",
        "            score += 20\n",
        "        elif 'mini' in specs_lower:\n",
        "            score += 10\n",
        "\n",
        "        # Score based on features; looks are important too!\n",
        "        if 'tempered glass' in specs_lower:\n",
        "            score += 15\n",
        "        if 'rgb' in specs_lower:\n",
        "            score += 10\n",
        "        if 'usb-c' in specs_lower or 'usb 3' in specs_lower:\n",
        "            score += 15\n",
        "\n",
        "        # Score based on cooling support; the more cooling support offered, the\n",
        "        # less likely thermal throttling (TJMax) is to occur\n",
        "        fan_match = re.search(r'(\\d+)\\s*fans?', specs, re.IGNORECASE)\n",
        "        if fan_match:\n",
        "            score += int(fan_match.group(1)) * 5\n",
        "\n",
        "    # Cooling scoring\n",
        "    elif any(cooling_term in part_type.lower() for cooling_term in [\"fan\", \"cooler\", \"cooling\", \"aio\"]):\n",
        "        # Score based on size; higher surface area of fins means more heat transfer\n",
        "        mm_match = re.search(r'(\\d+)\\s*mm', specs, re.IGNORECASE)\n",
        "        if mm_match:\n",
        "            score += int(mm_match.group(1)) / 10\n",
        "\n",
        "        # Score based on type; a liquid cooler will most likely be more\n",
        "        # efficient in heat transfer, having external radiator normally means more fins\n",
        "        if 'aio' in specs_lower or 'liquid' in specs_lower or 'water' in specs_lower:\n",
        "            score += 30\n",
        "        elif 'air' in specs_lower:\n",
        "            score += 15\n",
        "\n",
        "        # Score based on RGB; looks are important too!\n",
        "        if 'rgb' in specs_lower:\n",
        "            score += 10\n",
        "\n",
        "    # Generic scoring for all part types based on specs length\n",
        "    # If specific metrics aren't found, we'll assume more text means more features\n",
        "    # Although that's a hacky solution needing improvements since it's an easy way to game the system...\n",
        "    score += len(specs) * 0.01\n",
        "\n",
        "    return score"
      ],
      "metadata": {
        "id": "3oXaEIXxdxYj",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-21T02:46:47.384591Z",
          "iopub.execute_input": "2025-04-21T02:46:47.384977Z",
          "iopub.status.idle": "2025-04-21T02:46:47.412752Z",
          "shell.execute_reply.started": "2025-04-21T02:46:47.384954Z",
          "shell.execute_reply": "2025-04-21T02:46:47.411747Z"
        }
      },
      "outputs": [],
      "execution_count": 26
    },
    {
      "cell_type": "code",
      "source": [
        "def rank_options_available(parts_data: Dict[str, List[Dict[str, str]]], budget: float) -> List[Dict[str, Any]]:\n",
        "    \"\"\"\n",
        "    Rank parts based on specifications and price, within budget.\n",
        "    Returns a list of top three ranked parts with calculated metrics.\n",
        "    \"\"\"\n",
        "    all_parts = []\n",
        "\n",
        "    for part_type, parts in parts_data.items():\n",
        "        for part in parts:\n",
        "            price = parse_price(part['price'])\n",
        "            if price is None:\n",
        "                continue  # Skip parts with no price\n",
        "\n",
        "            if price > budget:\n",
        "                continue  # Skip parts over budget\n",
        "\n",
        "            specs_score = extract_specs_score(part['specifications'], part_type)\n",
        "\n",
        "            # Calculate value score (performance per dollar)\n",
        "            value_score = specs_score / price if price > 0 else 0\n",
        "\n",
        "            all_parts.append({\n",
        "                'name': part['name'],\n",
        "                'price': part['price'],\n",
        "                'raw_price': price,\n",
        "                'specifications': part['specifications'],\n",
        "                'purchase link': part['purchase link'],\n",
        "                'part_type': part_type,\n",
        "                'specs_score': specs_score,\n",
        "                'value_score': value_score\n",
        "            })\n",
        "\n",
        "    # Sort by specs score first (higher is better)\n",
        "    all_parts.sort(key=lambda x: x['specs_score'], reverse=True)\n",
        "\n",
        "    # Get top three parts\n",
        "    top_parts = all_parts[:3]\n",
        "\n",
        "    # Clean up the output by removing temporary fields\n",
        "    for part in top_parts:\n",
        "        part.pop(\"raw_price\", None)\n",
        "        part.pop(\"specs_score\", None)\n",
        "        part.pop(\"value_score\", None)\n",
        "\n",
        "    return top_parts\n",
        "\n",
        "\n",
        "def process_parts(json_data: str, budget: float) -> str:\n",
        "    \"\"\"\n",
        "    Main function to process parts data and return top three ranked parts.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Parse JSON string to dictionary\n",
        "        parts_dict = json.loads(json_data)\n",
        "\n",
        "        # Rank parts\n",
        "        top_parts = {}\n",
        "        for part_type, part_data in parts_dict.items():\n",
        "            top_parts[part_type] = rank_options_available({part_type: part_data}, budget)\n",
        "\n",
        "        # Convert results back to JSON\n",
        "        result_json = json.dumps(top_parts, indent=4, ensure_ascii=False)\n",
        "        return result_json\n",
        "\n",
        "    except json.JSONDecodeError:\n",
        "        return json.dumps({'error': 'Invalid JSON input'})\n",
        "    except Exception as e:\n",
        "        return json.dumps({'error': str(e)})"
      ],
      "metadata": {
        "id": "ZVBSp7jdKVTm",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-21T02:46:47.413694Z",
          "iopub.execute_input": "2025-04-21T02:46:47.413963Z",
          "iopub.status.idle": "2025-04-21T02:46:47.430864Z",
          "shell.execute_reply.started": "2025-04-21T02:46:47.413944Z",
          "shell.execute_reply": "2025-04-21T02:46:47.429949Z"
        }
      },
      "outputs": [],
      "execution_count": 27
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ðŸ”§ Step 6: Tooling Up â€“ LangChain Tools for the Assistant\n",
        "\n",
        "Here we define a suite of tools the assistant can call to take real-world actions:\n",
        "\n",
        "- **Search-related tools**:  \n",
        "  - `search_prebuilt()` â€“ Find ready-made devices; aided by helper function `search_prebuilt_devices()`\n",
        "  - `search_task_requirements()` â€“ Understand and return task-specific hardware needs  \n",
        "  - `search_custom_parts()` â€“ Find individual parts within budget; aided by helper function `search_individual_part()`\n",
        "\n",
        "- **Planner tools**:  \n",
        "  - `update_plans()` â€“ Add/update the user's needs  \n",
        "  - `clear_plan()` â€“ Reset everything and start fresh  \n",
        "\n",
        "- **Scoring tools**:  \n",
        "  - `rank_prebuilds()` â€“ Rank top 3 prebuilt PCs  \n",
        "  - `rank_parts()` â€“ Rank individual PC components  \n",
        "\n",
        "Each tool is modular; all the searching tools are stateless, while the planning and scoring/optimizing tools are stateful &mdash; making changes to the state through the specialized nodes defined later.\n",
        "\n",
        "LangGraph then binds these tools to the LLM to make them available for Gemini to dynamically call them at the right time.\n"
      ],
      "metadata": {
        "id": "3V95ZZQcCDlm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Stateless tool - Search online for pre-built devices\n",
        "\n",
        "# Tool helper for the search_custom_parts() tool\n",
        "@retry.Retry(predicate=is_retriable)\n",
        "def search_prebuilt_devices(budget: str, requirements: list[str]) -> str:\n",
        "    newline_char = '\\n'\n",
        "    search_config = types.GenerateContentConfig(\n",
        "        tools=[types.Tool(google_search=types.GoogleSearch())],\n",
        "        temperature=0.0\n",
        "    )\n",
        "\n",
        "    prompt = f'''\n",
        "You are a computer hardware specialist who always responds in valid JSON. Search online\n",
        "for pre-built laptops or desktops based on the requirements provided below and return\n",
        "the results in JSON format with name, price, specifications, and purchase link for each\n",
        "device. Find at least 3-6 options for the device in question.\n",
        "\n",
        "JSON Structure to follow:\n",
        "Return a list of device objects, where each object has three fields: name, price, and specifications.\n",
        "\n",
        "Key points to note:\n",
        "- Please answer the following question using ONLY information found in the provided web search results.\n",
        "- Rely exclusively on the real-time search results to answer.\n",
        "\n",
        "Device Requirements:\n",
        "    1. Budget: {budget}\n",
        "    2. Requirements: {newline_char + (newline_char.join([((' ' * 8) + '- ' + req) for req in requirements]))}\n",
        "'''\n",
        "\n",
        "    response = client.models.generate_content(\n",
        "        model=MAIN_MODEL,\n",
        "        contents=prompt,\n",
        "        config=search_config\n",
        "    )\n",
        "    rc = response.candidates[0]\n",
        "    return rc.content.parts[0].text\n",
        "\n",
        "\n",
        "# Actual tool definition\n",
        "@tool\n",
        "def search_prebuilt(budget: str, requirements: list[str]) -> str:\n",
        "    \"\"\"\n",
        "    Search for pre-built desktops or laptops fulfilling the user criteria.\n",
        "    Take the budget and requirements, and get Gemini to format it into a terse, concise query.\n",
        "    Then use search grounding to look for it and return the output as structured JSON with\n",
        "    price, brand, desktop/laptop, name, link to view more or buy\n",
        "    \"\"\"\n",
        "    print('CALLED TOOL: search_prebuilt')\n",
        "\n",
        "    return return_clean_json(search_prebuilt_devices, budget, requirements)"
      ],
      "metadata": {
        "id": "DxR35j6D_kBh",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-21T02:46:47.431970Z",
          "iopub.execute_input": "2025-04-21T02:46:47.432239Z",
          "iopub.status.idle": "2025-04-21T02:46:47.458943Z",
          "shell.execute_reply.started": "2025-04-21T02:46:47.432218Z",
          "shell.execute_reply": "2025-04-21T02:46:47.458072Z"
        }
      },
      "outputs": [],
      "execution_count": 28
    },
    {
      "cell_type": "code",
      "source": [
        "# Stateless tool - Search hardware requirements for a particular task\n",
        "@tool\n",
        "def search_task_requirements(special_task_requirement: str) -> list[str]:\n",
        "    '''\n",
        "    Search hardware requirements for a particular task.\n",
        "    Takes in a string describing the task and returns a list of hardware specs.\n",
        "    '''\n",
        "    print('CALLED TOOL: search_task_requirements')\n",
        "    print('Special use case:', special_task_requirement)\n",
        "\n",
        "    sys_prompt = '''\n",
        "You are a computer hardware specialist. The user has some hardware intensive tasks they want\n",
        "to perform and they want to know the hardware specs their device will need for them to be able\n",
        "to do what they want. For gaming, it might be helpful to consider minimum and recommended specs.\n",
        "Search online for these specs needed and formulate that into a structured, comma-separated list\n",
        "of concise individual requirements. Respond ONLY WITH a concise list of requirements separated by\n",
        "commas, add no preface or conclusion.\n",
        "\n",
        "Key points to note:\n",
        "- Please answer the following question using ONLY information found in the provided web search results.\n",
        "- Rely exclusively on the real-time search results to answer. Do not provide anything that is not found online.\n",
        "\n",
        "Example answer 1:\n",
        "Windows 10/11 64-bit, AMD Ryzen 7 CPU 5700X CPU, 16GB RAM, AMD Radeon RX 6700 XT, 170 GB SSD\n",
        "\n",
        "Example answer 2:\n",
        "Windows 11 64-bit, 8GB RAM, Intel Core i7-10700 CPU, NVIDIA GeForce RTX 2080 (8GB VRAM) GPU\n",
        "\n",
        "Example answer 3:\n",
        "AMD Ryzen 5 3600 @ 3.6 GHz or Intel Core i7-8700K @ 3.7 GHz or better, 8 GB RAM, AMD RX 570 (4 GB) or NVIDIA GeForce GTX 1060 (6 GB) or better, 85.5 GB storage\n",
        "'''\n",
        "\n",
        "    response = client.models.generate_content(\n",
        "        model=MAIN_MODEL,\n",
        "        contents=f'The use case that the hardware is needed for: {special_task_requirement}',\n",
        "        config=types.GenerateContentConfig(\n",
        "            tools=[types.Tool(google_search=types.GoogleSearch())],\n",
        "            system_instruction=sys_prompt,\n",
        "            temperature=0.0\n",
        "        )\n",
        "    )\n",
        "\n",
        "    rc = response.candidates[0]\n",
        "    hardware_needed = rc.content.parts[0].text\n",
        "    if len(hardware_needed) > 0:\n",
        "        return hardware_needed.split(', ')\n",
        "    return hardware_needed"
      ],
      "metadata": {
        "id": "_CrjI_sD-Cp1",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-21T02:46:47.459991Z",
          "iopub.execute_input": "2025-04-21T02:46:47.460345Z",
          "iopub.status.idle": "2025-04-21T02:46:47.489131Z",
          "shell.execute_reply.started": "2025-04-21T02:46:47.460319Z",
          "shell.execute_reply": "2025-04-21T02:46:47.488193Z"
        }
      },
      "outputs": [],
      "execution_count": 29
    },
    {
      "cell_type": "code",
      "source": [
        "# Stateless tool - Lookup parts required internally using RAG and rules of thumb\n",
        "@tool\n",
        "def lookup_parts_needed(requirements: Iterable[str]) -> list[str]:\n",
        "    \"\"\"\n",
        "    Look to improve the suggested hardware from search_task_requirements using the rules outlined in system_prompt.\n",
        "    Return list of components that will meet the requirements and conform to the rules.\n",
        "    \"\"\"\n",
        "    print('CALLED TOOL: lookup_parts_needed')\n",
        "\n",
        "    static_prompt = '''\n",
        "You are a computer hardware specialist, able to provide PC hardware recommendations based off of\n",
        "requirements you will be presented and using the provided text. When recommending parts you should conform to the\n",
        "following rules:\n",
        "RULES:\n",
        "1. Select Graphics Processing Unit (GPU) that's ~50% of the total budget\n",
        "2. Select a Central Processing Unit (CPU) that does not bottleneck GPU\n",
        "3. Select motherboard (MB) based on quality of above 2 components (budget vs pro build)\n",
        "4. Select Random Access Memory (RAM) related to Overclock (OC) specs on CPU and MB\n",
        "    i. Don't suggest RAM that has OC when MB/CPU does not support it, prioritize price in this situation\n",
        "    ii. Ensure RAM is on MB Qualified Vendors List (QVL)\n",
        "5. Select a Power Supply Unit (PSU) that can support all components power draw in Watts (W)\n",
        "    i. Prioritize modular power supplies should budget allow\n",
        "    ii. Minimum 10% headroom over expected power draw to account for cooler/storage/fans etc\n",
        "    iii. Include optional 20-30% headroom for future upgrades if budget allows\n",
        "        a. Eg components expected power draw is 800W so suggest ~900W PSU\n",
        "6. Selecting Storage\n",
        "    i. Prioritize traditional 2.5 Solid State Drives (SSDs) for budget builds with as much storage as possible\n",
        "    ii. Suggest M.2 SSDs for pro builds\n",
        "    iii. Hard Disk Drives (HDDs) only if minimum storage cannot be met within budget\n",
        "7. Select a cooler that is both appropriate for the build and budget\n",
        "    i. eg not air cooler for hotter CPUs (Intel 14900K, etc.)\n",
        "8. Select a case that will meet the GPU length requirements, MB type (ATX, ITX etc), and number of SSDs/HDDs suggested\n",
        "9. Select fans depending on number of fans that come with the case\n",
        "    i. Aim to have minimum 3 (2 intake; 1 exhaust)\n",
        "    ii. Should budget allow, fill all fan slots on case\n",
        "10. Any money left over from the budget should be used to improve the computer. For example, picking a better GPU,\n",
        "    CPU, more storage etc.\n",
        "\n",
        "You must respond with a list of components, separated by commas with no additional information or comment. This list\n",
        "must include a CPU, GPU, PSU, case, motherboard, fans, and storage unless the user has provided one of these already\n",
        "as existing hardware.\n",
        "'''\n",
        "\n",
        "    dynamic_prompt = 'The parts recommended must be able to accommodate the following requirements and be supported by their passages:'\n",
        "\n",
        "    for req in requirements:\n",
        "        dynamic_prompt += f'REQUIREMENT: {req}\\n'\n",
        "        passages = get_query_result(req)\n",
        "\n",
        "        for passage in passages:\n",
        "            passage_oneline = passage.replace('\\n', ' ')\n",
        "            dynamic_prompt += f'PASSAGE: {passage_oneline}\\n'\n",
        "\n",
        "    answer = client.models.generate_content(\n",
        "        model=MAIN_MODEL,\n",
        "        contents=dynamic_prompt,\n",
        "        config=types.GenerateContentConfig(\n",
        "            system_instruction=static_prompt,\n",
        "            temperature=0.2\n",
        "        )\n",
        "    )\n",
        "\n",
        "    answer = answer.text.replace('\\n', '')\n",
        "    return [s for s in answer.split(', ')]"
      ],
      "metadata": {
        "id": "EwN9E2H-STbc",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-21T02:46:47.490002Z",
          "iopub.execute_input": "2025-04-21T02:46:47.490324Z",
          "iopub.status.idle": "2025-04-21T02:46:47.514128Z",
          "shell.execute_reply.started": "2025-04-21T02:46:47.490297Z",
          "shell.execute_reply": "2025-04-21T02:46:47.513341Z"
        }
      },
      "outputs": [],
      "execution_count": 30
    },
    {
      "cell_type": "code",
      "source": [
        "# Stateless tool - Search online for custom parts for the build\n",
        "\n",
        "# Tool helper for the search_custom_parts() tool\n",
        "@retry.Retry(predicate=is_retriable)\n",
        "def search_individual_part(budget: str, part_needed: str) -> str:\n",
        "    \"\"\"\n",
        "    Search for an individual PC part. Takes in the budget and the parts needed and\n",
        "    return the needed part type along with the found recommendations in JSON.\n",
        "    \"\"\"\n",
        "    print(f'CALLED TOOL-HELPER: search_individual_part: {part_needed}')\n",
        "\n",
        "    sys_prompt = f'''\n",
        "You are a computer hardware specialist who always responds in valid JSON. Search online\n",
        "for cost-effective computer hardware parts that match the part that the user is looking for.\n",
        "Be sure to keep in mind that the budget given is the user's budget for all the parts put\n",
        "together. Thus, look for the most cost-effective parts since the other parts will also require\n",
        "parts of the budget. Recommend the latest parts that match this criteria and return the results\n",
        "in JSON format with name, price, specifications, and purchase link for each part. Find at least\n",
        "2-4 options for this piece of hardware.\n",
        "\n",
        "JSON Structure to follow:\n",
        "Return a list of part objects, where each object has four fields: name, price, specifications,\n",
        "and purchase link.\n",
        "\n",
        "Key points to note:\n",
        "- You should focus your search to the US and assume that the budget is in USD ($).\n",
        "- Please answer the following question using ONLY information found in the provided web search results.\n",
        "- Rely exclusively on the real-time search results to answer.\n",
        "- Respond only in concise JSON as requested, and do not add any preface or conclusions.\n",
        "'''\n",
        "\n",
        "    search_config = types.GenerateContentConfig(\n",
        "        tools=[types.Tool(google_search=types.GoogleSearch())],\n",
        "        temperature=0.0,\n",
        "        system_instruction=sys_prompt\n",
        "    )\n",
        "\n",
        "    response = client.models.generate_content(\n",
        "        model=MAIN_MODEL,\n",
        "        contents=f'Look for {part_needed} for a budget of {budget}.',\n",
        "        config=search_config\n",
        "    )\n",
        "    rc = response.candidates[0]\n",
        "    return rc.content.parts[0].text\n",
        "\n",
        "\n",
        "# Actual tool definition\n",
        "@tool\n",
        "def search_custom_parts(budget: str, parts_needed: Iterable[str] = []) -> Dict[str, list[Dict[str, Any]]]:\n",
        "    '''\n",
        "    Takes in a budget and a list of parts needed and calls the helper method\n",
        "    to find several parts matching the budget.\n",
        "    '''\n",
        "    print('CALLED TOOL: search_custom_parts')\n",
        "\n",
        "    parts_found = {}\n",
        "    for part_needed in parts_needed:\n",
        "        part_data = return_clean_json(search_individual_part, budget, part_needed)\n",
        "        parts_found[part_needed] = json.loads(part_data)\n",
        "\n",
        "    return json.dumps(parts_found, indent=4, ensure_ascii=False)"
      ],
      "metadata": {
        "id": "_Hjm6RUkVbsJ",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-21T02:46:47.515291Z",
          "iopub.execute_input": "2025-04-21T02:46:47.515625Z",
          "iopub.status.idle": "2025-04-21T02:46:47.540874Z",
          "shell.execute_reply.started": "2025-04-21T02:46:47.515598Z",
          "shell.execute_reply": "2025-04-21T02:46:47.539980Z"
        }
      },
      "outputs": [],
      "execution_count": 31
    },
    {
      "cell_type": "code",
      "source": [
        "# Tool signatures for planning the build\n",
        "# Functionality defined in pc_planner_node\n",
        "@tool\n",
        "def update_plans(requirements: Iterable[str], budget: Optional[str] = None) -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Adds or modifies the device requirements and budget.\n",
        "    Returns a confirmation of the budget and requirements that were just added to state.\n",
        "    \"\"\"\n",
        "\n",
        "\n",
        "@tool\n",
        "def clear_plan():\n",
        "    \"\"\"\n",
        "    Removes all requirements and budget information and resets to blank slate.\n",
        "    \"\"\""
      ],
      "metadata": {
        "id": "SdoWxZlJIoUb",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-21T02:46:47.541965Z",
          "iopub.execute_input": "2025-04-21T02:46:47.542287Z",
          "iopub.status.idle": "2025-04-21T02:46:47.569610Z",
          "shell.execute_reply.started": "2025-04-21T02:46:47.542265Z",
          "shell.execute_reply": "2025-04-21T02:46:47.568616Z"
        }
      },
      "outputs": [],
      "execution_count": 32
    },
    {
      "cell_type": "code",
      "source": [
        "# Tool signatures for recommending parts for a planned build\n",
        "# Functionality defined in optimize_build_node\n",
        "@tool\n",
        "def rank_parts(recommended_parts: str = '') -> str:\n",
        "    \"\"\"\n",
        "    Takes in a list of parts in JSON and a budget and returns the most\n",
        "    performant yet price-optimal parts for each part.\n",
        "    Modifies state by adding the parts to the recommendations list.\n",
        "    \"\"\"\n",
        "\n",
        "\n",
        "@tool\n",
        "def rank_prebuilds(recommended_devices: str = '') -> str:\n",
        "    \"\"\"\n",
        "    Takes in a list of prebuilt devices in JSON and a budget and returns the top\n",
        "    three most performant yet price-optimal devices, also in JSON.\n",
        "    Modifies state by adding the devices to the recommendations list.\n",
        "    \"\"\""
      ],
      "metadata": {
        "id": "3BVtk0cTIroB",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-21T02:46:47.570536Z",
          "iopub.execute_input": "2025-04-21T02:46:47.570873Z",
          "iopub.status.idle": "2025-04-21T02:46:47.593102Z",
          "shell.execute_reply.started": "2025-04-21T02:46:47.570848Z",
          "shell.execute_reply": "2025-04-21T02:46:47.592090Z"
        }
      },
      "outputs": [],
      "execution_count": 33
    },
    {
      "cell_type": "code",
      "source": [
        "# Tool grouped on nodes\n",
        "planner_tools = [update_plans, clear_plan]\n",
        "builder_tools = [rank_parts, rank_prebuilds]\n",
        "\n",
        "# Tools Config\n",
        "auto_tools = [search_prebuilt, search_task_requirements, lookup_parts_needed, search_custom_parts]\n",
        "tool_node = ToolNode(auto_tools)\n",
        "\n",
        "# Tool binding\n",
        "llm_with_tools = llm.bind_tools(auto_tools + planner_tools + builder_tools)"
      ],
      "metadata": {
        "id": "M-9DAAU6QH6i",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-21T02:46:47.593985Z",
          "iopub.execute_input": "2025-04-21T02:46:47.594248Z",
          "iopub.status.idle": "2025-04-21T02:46:47.634084Z",
          "shell.execute_reply.started": "2025-04-21T02:46:47.594222Z",
          "shell.execute_reply": "2025-04-21T02:46:47.633225Z"
        }
      },
      "outputs": [],
      "execution_count": 34
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ðŸ” Step 7: Planning and Optimization Nodes for State Management\n",
        "\n",
        "We follow the heuristic taught to us during the Hackathon of updating state only through controlled, stateful nodes, rather than handing automatic tool-calling directly to the LLM.\n",
        "\n",
        "Thus, these LangGraph nodes define the functionality for the tools that mutate and update the assistant's state.\n",
        "\n",
        "- `pc_planner_node`: Takes new requirements and budget from the user and updates the assistant's memory. `update_plans()` and `clear_plan()` have their functionality defined here.\n",
        "- `optimize_build_node`: Uses ranking tools to turn search results into final recommendations. The `rank_prebuilds()` and `rank_parts()` fall under its jurisdiction.\n",
        "\n",
        "These nodes **mutate state** in a clean, deterministic way, allowing LangGraph to \"think\" through the problem and carry context forward, adjusting the build plan intelligently as it goes along."
      ],
      "metadata": {
        "id": "JYBw1WseDW1M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Build planner node\n",
        "def pc_planner_node(state: PCBuilderState) -> PCBuilderState:\n",
        "    \"\"\"This is where the requirements and budget within state get manipulated.\"\"\"\n",
        "\n",
        "    tool_msg = state.get(\"messages\", [])[-1]\n",
        "    requirements_state = state.get(\"requirements\", [])\n",
        "    budget_state = state.get('budget', None)\n",
        "    recommendations_state = state.get('recommendations', [])\n",
        "    outbound_msgs = []\n",
        "    build_complete = state.get('build_complete', False)\n",
        "\n",
        "    for tool_call in tool_msg.tool_calls:\n",
        "        if tool_call['name'] == 'update_plans':\n",
        "            print('CALLED TOOL: update_plans')\n",
        "\n",
        "            args_given = tool_call['args']\n",
        "            requirements_arg = budget_arg = None\n",
        "            if 'requirements' in args_given:\n",
        "                requirements_arg = args_given['requirements']\n",
        "            if 'budget' in args_given:\n",
        "                budget_arg = args_given['budget']\n",
        "\n",
        "            # If budget is None and nothing exists in state, raise an error.\n",
        "            # If there is something in state, then just don't update it.\n",
        "            # Otherwise always update the budget.\n",
        "            if budget_arg is None or len(budget_arg) < 1:\n",
        "                if budget_state is None or len(budget_state) < 1:\n",
        "                    raise ValueError(f'Budget is missing in tool call as well as state!')\n",
        "            else:\n",
        "                budget_state = budget_arg\n",
        "\n",
        "            if requirements_arg is None or len(requirements_arg) < 1:\n",
        "                if requirements_state is None or len(requirements_state) < 1:\n",
        "                    raise ValueError('Requirements are missing in tool call as well as state!')\n",
        "            else:\n",
        "                requirements_state = [requirement for requirement in requirements_arg]\n",
        "\n",
        "            response = {\n",
        "                'budget': budget_arg if budget_arg is not None else budget_state,\n",
        "                'requirements': requirements_arg\n",
        "            }\n",
        "\n",
        "        elif tool_call['name'] == 'clear_plan':\n",
        "            print('CALLED TOOL: clear_plan')\n",
        "            requirements_state = []\n",
        "            budget_state = None\n",
        "            response = None\n",
        "\n",
        "        else:\n",
        "            raise NotImplementedError(f'Unknown tool call: {tool_call[\"name\"]}')\n",
        "\n",
        "        # Record the tool results as tool messages.\n",
        "        outbound_msgs.append(\n",
        "            ToolMessage(\n",
        "                content=response,\n",
        "                name=tool_call[\"name\"],\n",
        "                tool_call_id=tool_call[\"id\"],\n",
        "            )\n",
        "        )\n",
        "\n",
        "    return {\n",
        "        'messages': outbound_msgs,\n",
        "        'requirements': requirements_state,\n",
        "        'budget': budget_state,\n",
        "        'recommendations': recommendations_state,\n",
        "        'build_complete': build_complete\n",
        "    }"
      ],
      "metadata": {
        "id": "9BmdR5L5I0-i",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-21T02:46:47.638181Z",
          "iopub.execute_input": "2025-04-21T02:46:47.638442Z",
          "iopub.status.idle": "2025-04-21T02:46:47.648172Z",
          "shell.execute_reply.started": "2025-04-21T02:46:47.638422Z",
          "shell.execute_reply": "2025-04-21T02:46:47.647272Z"
        }
      },
      "outputs": [],
      "execution_count": 35
    },
    {
      "cell_type": "code",
      "source": [
        "# Parts recommender node (based on devised plan and recommended devices)\n",
        "def optimize_build_node(state: PCBuilderState) -> PCBuilderState:\n",
        "    \"\"\"This is where the recommendations state gets manipulated.\"\"\"\n",
        "\n",
        "    tool_msg = state.get(\"messages\", [])[-1]\n",
        "    requirements_state = state.get(\"requirements\", [])\n",
        "    budget_state = state.get('budget', None)\n",
        "    recommendations_state = state.get('recommendations', [])\n",
        "    outbound_msgs = []\n",
        "    build_complete = state.get('build_complete', False)\n",
        "\n",
        "    for tool_call in tool_msg.tool_calls:\n",
        "        args_available = tool_call['args']\n",
        "\n",
        "        if tool_call['name'] == 'rank_parts':\n",
        "            print('CALLED TOOL: rank_parts')\n",
        "            if 'recommended_parts' not in args_available:\n",
        "                raise ValueError('There are no recommended parts available to rank!')\n",
        "\n",
        "            recommended_parts = args_available['recommended_parts']\n",
        "            if budget_state is None or len(budget_state) < 1:\n",
        "                raise ValueError(f'An invalid budget was found within state!')\n",
        "\n",
        "            print('\\nThis is the custom parts recommendations json provided:\\n', recommended_parts, '\\n')\n",
        "            recommendations_state = process_parts(strip_json_wrapper(recommended_parts), parse_price(budget_state))\n",
        "            response = recommendations_state\n",
        "\n",
        "        elif tool_call['name'] == 'rank_prebuilds':\n",
        "            print('CALLED TOOL: rank_prebuilds')\n",
        "            recommended_devices = tool_call['args']['recommended_devices']\n",
        "            if budget_state is None or len(budget_state) < 1:\n",
        "                raise ValueError(f'An invalid budget was found within state!')\n",
        "\n",
        "            print('\\nThis is the prebuilds recommendations json provided:\\n', recommended_devices, '\\n')  # Comment back in to see in output console\n",
        "            try:\n",
        "                json.loads(recommended_devices)\n",
        "                recommendations_state = rank_devices(recommended_devices, parse_price(budget_state))\n",
        "                response = rank_devices(recommended_devices, parse_price(budget_state), True)\n",
        "            except json.JSONDecodeError:\n",
        "                print('The JSON string provided by the model was invalid. Will attempt to retry...')\n",
        "                recommendations_state = response = json.dumps({'error': 'The JSON string provided was invalid. Please recall the tool and return valid JSON instead.'}, ensure_ascii=False)\n",
        "\n",
        "        else:\n",
        "            raise NotImplementedError(f'Unknown tool call: {tool_call[\"name\"]}')\n",
        "\n",
        "        # Record the tool results as tool messages.\n",
        "        outbound_msgs.append(\n",
        "            ToolMessage(\n",
        "                content=response,\n",
        "                name=tool_call[\"name\"],\n",
        "                tool_call_id=tool_call[\"id\"],\n",
        "            )\n",
        "        )\n",
        "\n",
        "    return {\n",
        "        'messages': outbound_msgs,\n",
        "        'requirements': requirements_state,\n",
        "        'budget': budget_state,\n",
        "        'recommendations': recommendations_state,\n",
        "        'build_complete': build_complete\n",
        "    }"
      ],
      "metadata": {
        "id": "51rRWwKoI2yq",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-21T02:46:47.649186Z",
          "iopub.execute_input": "2025-04-21T02:46:47.649503Z",
          "iopub.status.idle": "2025-04-21T02:46:47.668956Z",
          "shell.execute_reply.started": "2025-04-21T02:46:47.649473Z",
          "shell.execute_reply": "2025-04-21T02:46:47.667951Z"
        }
      },
      "outputs": [],
      "execution_count": 36
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ðŸ’¬ Step 8: Nodes to Define the Core Conversation Loop\n",
        "\n",
        "The two most instrumental nodes are defined here. They allow the AI and the user to talk back-and-forth until the user is satisfied with their recommendations.\n",
        "\n",
        "- ðŸ¤– `chatbot_node`: Gemini handles user questions and generates tool calls.\n",
        "- ðŸ‘± `human_node`: Displays the latest message to the user and waits for their typed input.\n",
        "\n",
        "We also handle quitting (`q`, `exit`, etc.) in the Human node and keep the conversation loop flowing smoothly using LangGraph's stateful transitions.\n"
      ],
      "metadata": {
        "id": "ES4dPnumEjiV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def chatbot_node(state: PCBuilderState) -> PCBuilderState:\n",
        "    \"\"\"The chatbot itself. A simple wrapper around the model's own chat interface.\"\"\"\n",
        "    default_state = {'requirements': [], 'budget': None, 'recommendations': [], 'build_complete': False}\n",
        "\n",
        "    if state['messages']:\n",
        "        # If there are messages, continue the conversation with the model.\n",
        "        message_history = [PC_BUILDER_SYSINT] + state[\"messages\"]\n",
        "        new_output = llm_with_tools.invoke(message_history)\n",
        "    else:\n",
        "        # If there are no messages, welcome the user.\n",
        "        new_output = AIMessage(content=WELCOME_MSG)\n",
        "\n",
        "    # Setup some defaults, then override with whatever exists in state, and finally\n",
        "    # override with messages.\n",
        "    return default_state | state | {\"messages\": [new_output]}"
      ],
      "metadata": {
        "id": "lzsqL221mQUL",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-21T02:46:47.670035Z",
          "iopub.execute_input": "2025-04-21T02:46:47.670362Z",
          "iopub.status.idle": "2025-04-21T02:46:47.688405Z",
          "shell.execute_reply.started": "2025-04-21T02:46:47.670337Z",
          "shell.execute_reply": "2025-04-21T02:46:47.687624Z"
        }
      },
      "outputs": [],
      "execution_count": 37
    },
    {
      "cell_type": "code",
      "source": [
        "def human_node(state: PCBuilderState) -> PCBuilderState:\n",
        "    \"\"\"Display the last message from the model to the user, and receive their input.\"\"\"\n",
        "\n",
        "    # Uncomment the below lines if you want to inspect all the pieces of state as they get updated.\n",
        "    # print('\\nAll state until now:')\n",
        "    # for key, val in state.items():\n",
        "    #     print(key, '::', val)\n",
        "    #     print('\\n')\n",
        "    # print('\\nNow onto the messages:\\n')\n",
        "\n",
        "    last_msg = state['messages'][-1]\n",
        "    display(Markdown(f'**Model:**\\n\\n{last_msg.content}\\n\\n&nbsp;\\n\\n'))\n",
        "\n",
        "    display(Markdown('**User:**'))\n",
        "    user_input = input()\n",
        "    print()\n",
        "\n",
        "    # Does the user wish to quit?\n",
        "    if user_input.lower() in {'q', 'quit', 'exit', 'goodbye'}:\n",
        "        state['build_complete'] = True\n",
        "\n",
        "    return state | {'messages': [('user', user_input)]}"
      ],
      "metadata": {
        "id": "ooizLUht1wRS",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-21T02:46:47.689489Z",
          "iopub.execute_input": "2025-04-21T02:46:47.689839Z",
          "iopub.status.idle": "2025-04-21T02:46:47.710417Z",
          "shell.execute_reply.started": "2025-04-21T02:46:47.689818Z",
          "shell.execute_reply": "2025-04-21T02:46:47.709482Z"
        }
      },
      "outputs": [],
      "execution_count": 38
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ðŸ”€ Step 9: Dynamic Routing and Decision-Making\n",
        "\n",
        "We leverage LangGraph's **conditional branching** in the workflow graph.\n",
        "\n",
        "Our key transition logic:\n",
        "\n",
        "- If Gemini wants to call a tool â†’ go to `tools`\n",
        "- If the user updated their requirements or budget â†’ go to `pc_planner`\n",
        "- If there are new parts or builds to rank â†’ go to `optimize_build`\n",
        "- If the user wishes to quit â†’ go to `END`\n",
        "\n",
        "This keeps the assistant flexible, responsive, and smart.\n"
      ],
      "metadata": {
        "id": "7QJKXBSLFszF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Human to Exit OR Human to Chatbot; Conditional Edge Transition function\n",
        "def maybe_exit_human_node(state: PCBuilderState) -> Literal[\"chatbot\", \"__end__\"]:\n",
        "    \"\"\"Route to the chatbot, unless it looks like the user is exiting.\"\"\"\n",
        "    if state.get(\"build_complete\", False):\n",
        "        return END\n",
        "    else:\n",
        "        return \"chatbot\""
      ],
      "metadata": {
        "id": "gqmUpWsu4wPZ",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-21T02:46:47.711486Z",
          "iopub.execute_input": "2025-04-21T02:46:47.711807Z",
          "iopub.status.idle": "2025-04-21T02:46:47.728010Z",
          "shell.execute_reply.started": "2025-04-21T02:46:47.711783Z",
          "shell.execute_reply": "2025-04-21T02:46:47.727113Z"
        }
      },
      "outputs": [],
      "execution_count": 39
    },
    {
      "cell_type": "code",
      "source": [
        "# Chatbot to Tools OR Chatbot to Human; Conditional Edge Transition function\n",
        "def maybe_route_to_tools(state: PCBuilderState) -> str:\n",
        "    if not (msgs := state.get('messages', [])):\n",
        "        raise ValueError(f'No messages found when parsing state: {state}')\n",
        "\n",
        "    # Only route based on the last message.\n",
        "    msg = msgs[-1]\n",
        "\n",
        "    if state.get('build_complete', False):\n",
        "        # If the user has no more questions or indicates satisfaction, complete the build\n",
        "        return END\n",
        "    elif hasattr(msg, 'tool_calls') and len(msg.tool_calls) > 0:\n",
        "        # When chatbot returns tool_calls, route to the 'tools' node\n",
        "        if any(tool['name'] in tool_node.tools_by_name.keys() for tool in msg.tool_calls):\n",
        "            return 'tools'\n",
        "        elif any(tool['name'] in [func.name for func in planner_tools] for tool in msg.tool_calls):\n",
        "            return 'pc_planner'\n",
        "        elif any(tool['name'] in [func.name for func in builder_tools] for tool in msg.tool_calls):\n",
        "            return 'optimize_build'\n",
        "        else:\n",
        "            raise ValueError('A nonexistent node was called.')\n",
        "    else:\n",
        "        return 'human'"
      ],
      "metadata": {
        "id": "gH9EZoIkBXWH",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-21T02:46:47.728910Z",
          "iopub.execute_input": "2025-04-21T02:46:47.729203Z",
          "iopub.status.idle": "2025-04-21T02:46:47.746879Z",
          "shell.execute_reply.started": "2025-04-21T02:46:47.729181Z",
          "shell.execute_reply": "2025-04-21T02:46:47.745809Z"
        }
      },
      "outputs": [],
      "execution_count": 40
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ðŸ§© Step 10: Building the Graph and Launching the Assistant\n",
        "\n",
        "Finally, we:\n",
        "\n",
        "- &nbsp;âˆ‘ &nbsp; Add all nodes to the LangGraph\n",
        "- ðŸšª&nbsp; Define the entrypoint (`chatbot_node`)\n",
        "- ðŸ”€ Define transitions between states\n",
        "- ðŸŽ¨ Render the graph visually\n",
        "- ðŸ” Set a high recursion limit (to allow longer context conversations)\n",
        "- ðŸ’¬ Launch the assistant and let users interact with it in real-time\n"
      ],
      "metadata": {
        "id": "yDqBt-E_Fukr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set up the initial graph based on our state definition.\n",
        "graph_builder = StateGraph(PCBuilderState)\n",
        "\n",
        "# Add all the nodes to the app graph.\n",
        "graph_builder.add_node('chatbot', chatbot_node)\n",
        "graph_builder.add_node('human', human_node)\n",
        "graph_builder.add_node('tools', tool_node)\n",
        "graph_builder.add_node('pc_planner', pc_planner_node)\n",
        "graph_builder.add_node('optimize_build', optimize_build_node)\n",
        "\n",
        "# Define the chatbot node as the app entrypoint.\n",
        "graph_builder.add_edge(START, 'chatbot')\n",
        "\n",
        "# Edge transitions\n",
        "graph_builder.add_conditional_edges('chatbot', maybe_route_to_tools)\n",
        "graph_builder.add_conditional_edges('human', maybe_exit_human_node)\n",
        "graph_builder.add_edge('tools', 'chatbot')\n",
        "graph_builder.add_edge('pc_planner', 'chatbot')\n",
        "graph_builder.add_edge('optimize_build', 'chatbot')\n",
        "\n",
        "chat_graph = graph_builder.compile()"
      ],
      "metadata": {
        "id": "vz-kOlcOwKry",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-21T02:46:47.747808Z",
          "iopub.execute_input": "2025-04-21T02:46:47.748033Z",
          "iopub.status.idle": "2025-04-21T02:46:47.777353Z",
          "shell.execute_reply.started": "2025-04-21T02:46:47.748014Z",
          "shell.execute_reply": "2025-04-21T02:46:47.776438Z"
        }
      },
      "outputs": [],
      "execution_count": 41
    },
    {
      "cell_type": "markdown",
      "source": [
        "If everything has gone well, this is how the LangGraph workflow should look:\n",
        "\n",
        "![LangGraph workflow.png](data:image/png;base64,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)\n",
        "\n",
        "Note: The library used here for visualization sometimes takes a long time to render and the program encounters a HTTP Connection lost exception. That is not a bug with our app, as running the exact same cell in a row successively will eventually work (with no other changes being made elsewhere).\n",
        "\n",
        "Thus, the below draw statement is being commented out, but when it works, it will look just like the image above."
      ],
      "metadata": {
        "id": "qrJj2uz49vs3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualize the graph created.\n",
        "# Below line is commented out due to intermittently causing errors. Comment back in to try it out.\n",
        "# Image(chat_graph.get_graph().draw_mermaid_png())"
      ],
      "metadata": {
        "id": "quysN6udx6zb",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-21T02:46:47.778387Z",
          "iopub.execute_input": "2025-04-21T02:46:47.778709Z",
          "iopub.status.idle": "2025-04-21T02:46:47.795875Z",
          "shell.execute_reply.started": "2025-04-21T02:46:47.778680Z",
          "shell.execute_reply": "2025-04-21T02:46:47.794986Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# The default recursion limit for traversing nodes is 25 - setting it higher means\n",
        "# being able to try multiple steps and chatting for longer.\n",
        "config = {\"recursion_limit\": 100}\n",
        "\n",
        "# Invocation of the agent workflow, starts the whole app.\n",
        "state = chat_graph.invoke({\"messages\": []}, config)\n",
        "print('Program successfully exited.')\n",
        "\n",
        "# pprint(state)"
      ],
      "metadata": {
        "id": "VDcqNgjV0KES",
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## âœ… Conclusion\n",
        "\n",
        "This notebook demonstrates how you can use **LangGraph + Gemini + real-time search + a scoring engine** to build a practical, domain-specific agent that solves a real-world problem.\n",
        "\n",
        "You've seen how the agent:\n",
        "- Talks to users\n",
        "- Understands needs\n",
        "- Searches online\n",
        "- Ranks results\n",
        "- Returns complete recommendations â€” all while remembering context and adapting intelligently\n",
        "\n",
        "This is a great example of how Generative AI can move beyond text generation to become a full-fledged problem-solving agent, **allowing users to get help with a variety of domain-specific problems!** We would love to come do more at Google (and especially @ DeepMind!) and help with anything you may need, so if you like what you see, don't hesitate to look through our profiles! ðŸ˜‰\n",
        "\n",
        " \\- ðŸ¶ [Adam](https://www.linkedin.com/in/adam-steinmetz-6463a4240/) and ðŸ¤– [Daniel](https://theaughat.github.io/personal-portfolio/)"
      ],
      "metadata": {
        "id": "IGLdj5inGFW1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ðŸ§­ Next Steps\n",
        "\n",
        "- **Graphical User Interface (GUI):** Incorporate an interactive UI via *Gradio* or *Streamlit* to make it easier for our users to interact with the agent\n",
        "- **RAG-based Question Answering:** A new tool that detects whether the user is asking knowledge or installation related questions about the parts or specific builds, and query a knowledge base with embedded specifically focused on this\n",
        "- **Improved Online Searching:** We plan to improve our searching strategies so the solutions provided to the user are more robust and hallucination-free\n",
        "- **Improved Search Results:** Fix markdown generation for final recommendations and try to include images of the parts mentioned\n",
        "- **Better Price Comparisons and Indication:** For custom PCs, average out the prices of each part type and indicate the proportion of the budget needed for it.\n",
        "\n",
        "If you're interested in seeing these improvements and any new projects by us in the future, feel free to follow us using the links provided above!"
      ],
      "metadata": {
        "id": "FE_THs4q_ovS"
      }
    }
  ]
}